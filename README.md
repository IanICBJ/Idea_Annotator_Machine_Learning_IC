# Idea_Annotator_Machine_Learning_IC
Machine learning models for the Idea Annotator system, including BERT and T5 pipelines for automatic case-frame extraction from raw text.
#Idea Annotator – Machine Learning (BERT & T5)

This repository contains the machine learning components of the Idea Annotator Final Year Project. It includes all Jupyter notebooks, datasets, and evaluation pipelines used in Sprint 3 (prototype development) and Sprint 4 (baseline evaluation + model improvement tracks). The goal of this ML component is to automatically extract 5-slot case-frames — ACTOR, ACTION, OBJECT, LOCATION, TIME — from raw user text using BERT and T5 models.

# Overview (Integrated Summary)

This repository contains the machine learning component of the Idea Annotator FYP, where BERT and T5 models are trained to extract case-frame slots (ACTOR, ACTION, OBJECT, LOCATION, TIME) from text. Sprint 3 develops the prototype pipeline using clean, simple data split into train.jsonl, val.jsonl, and test.jsonl, located in Sprint_3_ML/Data_Required/, and also uses dataset.jsonl and dataset_full.jsonl where needed for combined or extended experiments. The notebooks in Sprint_3_ML/Jupyter_Notebooks/ train baseline BERT and T5 models. Sprint 4 upgrades the system to real, messy idea descriptions, starting from dataset_full.jsonl (the full annotated dataset exported from MongoDB), which is used in Sprint4_Track0_Fixed_Split.ipynb to generate the official dataset idea_annotator_sprint4_split_fixed.jsonl stored in Sprint_4_ML/Data_Required/. All Sprint 4 improvements—baseline evaluation, BERT decoding fixes (Track 1), and T5 JSON repair (Track 2)—are found in Sprint_4_ML/Jupyter_Notebooks/. In short: Sprint 3 = prototype models on clean data with train/val/test + dataset.jsonl; Sprint 4 = realistic data from dataset_full.jsonl + fixed split + model improvement tracks.

## Repository Structure
### Sprint_3_ML
Data_Required:
  - dataset.jsonl
  - dataset_full.jsonl
  - train.jsonl
  - val.jsonl
  - test.jsonl
Jupyter_Notebooks
BERT_Prototype.ipynb
T5_Prototype.ipynb

print_4_ML
Data_Required
dataset_full.jsonl
idea_annotator_sprint4_split_fixed.jsonl
 Jupyter_Notebooks/Sprint4_Track0_Fixed_Split.ipynb
Sprint4_Track1_BERT_Improvements.ipynb
Sprint4_Track2_T5_Improvements.ipynb

# Datasets Used in This Repository

# Sprint 3 – Data Files

- dataset.jsonl

  Combined clean dataset used in Sprint 3.

  Source file from which the three-way split is created or referenced in some experiments.

- train.jsonl, val.jsonl, test.jsonl

  Main Sprint 3 training, validation, and test files.

  Contain clean, simple sentences with final 5-slot labels.

  Used directly by the Sprint 3 BERT and T5 prototype notebooks for training and evaluation.

- dataset_full.jsonl

  Full annotated dataset exported from MongoDB.

  Used in Sprint 3 when testing or inspecting behaviour on the full set (e.g., bridging towards Sprint 4).

# Sprint 4 – Data Files

- dataset_full.jsonl

  Same full annotated dataset as above.

  Acts as the raw input for creating the official fixed split in Sprint 4.

- idea_annotator_sprint4_split_fixed.jsonl

  Official train/dev/test split file for Sprint 4.

  Generated by Sprint4_Track0_Fixed_Split.ipynb.

  Used by all Sprint 4 tracks (baseline, BERT improvements, T5 improvements) to ensure consistent evaluation.

# Sprint 3: Prototype Models on Clean Data

Sprint 3 builds the first working ML pipeline on a controlled, simplified dataset.

Key Points

Main training and evaluation rely on:

train.jsonl

val.jsonl

test.jsonl

dataset.jsonl is used as the combined clean source corpus for these splits and some experiments.

dataset_full.jsonl is available when testing on the larger annotated set.

# Models

BERT Prototype

Uses BIO tagging over the clean data.

Decodes token labels into spans and then into JSON frames.

T5 Prototype

Learns to map the clean sentence directly into a JSON string.

Provides an early test of generative text-to-JSON modelling.

Notebooks

Sprint_3_ML/Jupyter_Notebooks/BERT_Prototype.ipynb

Sprint_3_ML/Jupyter_Notebooks/T5_Prototype.ipynb

These notebooks show the full prototype workflow: data loading, training, prediction, decoding, and evaluation.

# Sprint 4: Realistic Data, Fixed Split, and Model Improvements

Sprint 4 moves from prototype conditions to realistic, noisy data exported from MongoDB.

1. Dataset and Split

Uses dataset_full.jsonl as the main raw dataset.

BIO tags are rebuilt against the final 5-slot schema.

Sprint4_Track0_Fixed_Split.ipynb creates idea_annotator_sprint4_split_fixed.jsonl, which is then used everywhere in Sprint 4.

2. Track 0 – Baselines

Evaluates baseline BERT and T5 on the fixed split.

Shows BERT is structurally reliable; T5 collapses without repair due to strict JSON formatting requirements.

3. Track 1 – BERT Improvements

Works on top of the same BERT model.

Improves:

BIO → span decoding

Slot-level cleaning rules

Minimal slot repair heuristics

Produces cleaner and more meaningful case-frames, even if strict Slot-F1 decreases.

4. Track 2 – T5 Improvements

Applies safe decoding and JSON repair to T5 outputs.

Recovers syntactically valid JSON from malformed generations.

Semantic accuracy remains limited, but the structure becomes evaluable.

Notebooks

Sprint_4_ML/Jupyter_Notebooks/Sprint4_Track0_Fixed_Split.ipynb

Sprint_4_ML/Jupyter_Notebooks/Sprint4_Track1_BERT_Improvements.ipynb

Sprint_4_ML/Jupyter_Notebooks/Sprint4_Track2_T5_Improvements.ipynb

# How to Use These Notebooks
Requirements

Python 3.10+

PyTorch

HuggingFace Transformers

Standard Python libraries (json, re, etc.)

Suggested Order

Look at Sprint 3 notebooks to understand the clean prototype setup using train.jsonl, val.jsonl, and test.jsonl.

Inspect how dataset.jsonl and dataset_full.jsonl relate to these splits.

Move to Sprint 4 Track 0 to see how dataset_full.jsonl is converted into the fixed split file.

Explore Track 1 and Track 2 to see how BERT and T5 behave on the real dataset and how their outputs are improved.

# Key Takeaways

Sprint 3 mainly trains on train.jsonl, val.jsonl, and test.jsonl, with dataset.jsonl and dataset_full.jsonl supporting additional experiments.

Sprint 4 standardises evaluation using dataset_full.jsonl and idea_annotator_sprint4_split_fixed.jsonl.

BERT is robust and improves significantly with better decoding and cleaning.

T5 needs heavy repair to be structurally valid and remains limited semantically under these conditions.
