{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b473d50b",
   "metadata": {},
   "source": [
    "## FYP Sprint 3 ML training\n",
    "\n",
    "### Ian Chia \n",
    "### 230746D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f66b9a",
   "metadata": {},
   "source": [
    "### Game Plan Idea\n",
    "\n",
    "1) Connect to the real data (350 annotated examples)\n",
    "\n",
    "2) Once connected train both models using the real data \n",
    "\n",
    "3) Compare and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9988b",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66966eb0",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bbe2d",
   "metadata": {},
   "source": [
    "### 1) Install & Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9126a1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import json, re, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "from jsonschema import validate, ValidationError\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import json, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b94fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.15.3-cp39-cp39-win_amd64.whl.metadata (22 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.11.4-cp39-cp39-win_amd64.whl.metadata (42 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading pymongo-4.15.3-cp39-cp39-win_amd64.whl (757 kB)\n",
      "   ---------------------------------------- 0.0/757.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 757.1/757.1 kB 3.5 MB/s eta 0:00:00\n",
      "Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading orjson-3.11.4-cp39-cp39-win_amd64.whl (131 kB)\n",
      "Installing collected packages: python-dotenv, orjson, dnspython, pymongo\n",
      "\n",
      "   ---------------------------------------- 0/4 [python-dotenv]\n",
      "   ---------------------------------------- 0/4 [python-dotenv]\n",
      "   ---------------------------------------- 0/4 [python-dotenv]\n",
      "   ---------- ----------------------------- 1/4 [orjson]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   -------------------- ------------------- 2/4 [dnspython]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ------------------------------ --------- 3/4 [pymongo]\n",
      "   ---------------------------------------- 4/4 [pymongo]\n",
      "\n",
      "Successfully installed dnspython-2.7.0 orjson-3.11.4 pymongo-4.15.3 python-dotenv-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo python-dotenv orjson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa5aa4",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f8fa8d",
   "metadata": {},
   "source": [
    "### 2) Set the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441f5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = \"mongodb://localhost:27017\"\n",
    "DB_NAME   = \"FYP\"\n",
    "COLL_NAME = \"Annotation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6df905",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f8930",
   "metadata": {},
   "source": [
    "### 3) Query Mongo and preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cddc555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 327\n",
      "{'sentence_id': 'S001', 'sentence_name': 'None', 'original_sentence': 'A selfish, whiny teenaged girl', 'owner_id': 'ianchia', 'owner_username': 'ianchia', 'shared_with': [], 'tokens': [{'token_id': 0, 'text': 'A', 'pos': 'DET', 'label': '', 'type': ''}, {'token_id': 1, 'text': 'selfish', 'pos': 'ADJ', 'label': '', 'type': ''}, {'token_id': 2, 'text': ',', 'pos': 'PUNCT', 'label': '', 'type': ''}, {'token_id': 3, 'text': 'whiny', 'pos': 'ADJ', 'label': '', 'type': ''}, {'token_id': 4, 'text': 'teenaged', 'pos': 'ADJ', 'label': '', 'type': ''}, {'token_id': 5, 'text': 'girl', 'pos': 'NOUN', 'label': 'Entity', 'type': ''}], 'ACTIONS': [], 'ENTITIES': [{'entity_id': 'e001', 'text': 'girl', 'token_id': [5], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': '', 'ADJ': {'text': 'teenaged', 'token_id': [4]}}}]}\n",
      "{'sentence_id': 'S002', 'sentence_name': 'None', 'original_sentence': 'When a character delivers a speech so powerful that it emotionally moves the others to take action and not lose hope', 'owner_id': 'ianchia', 'owner_username': 'ianchia', 'shared_with': [], 'tokens': [{'token_id': 0, 'text': 'When', 'pos': 'SCONJ', 'label': '', 'type': ''}, {'token_id': 1, 'text': 'a', 'pos': 'DET', 'label': '', 'type': ''}, {'token_id': 2, 'text': 'character', 'pos': 'NOUN', 'label': 'Entity', 'type': 'Person'}, {'token_id': 3, 'text': 'delivers', 'pos': 'VERB', 'label': 'Action', 'type': ''}, {'token_id': 4, 'text': 'a', 'pos': 'DET', 'label': '', 'type': ''}, {'token_id': 5, 'text': 'speech', 'pos': 'NOUN', 'label': 'Entity', 'type': 'Object'}, {'token_id': 6, 'text': 'so', 'pos': 'ADV', 'label': '', 'type': ''}, {'token_id': 7, 'text': 'powerful', 'pos': 'ADJ', 'label': '', 'type': ''}, {'token_id': 8, 'text': 'that', 'pos': 'SCONJ', 'label': '', 'type': ''}, {'token_id': 9, 'text': 'it', 'pos': 'PRON', 'label': '', 'type': ''}, {'token_id': 10, 'text': 'emotionally', 'pos': 'ADV', 'label': '', 'type': ''}, {'token_id': 11, 'text': 'moves', 'pos': 'VERB', 'label': 'Action', 'type': ''}, {'token_id': 12, 'text': 'the', 'pos': 'DET', 'label': '', 'type': ''}, {'token_id': 13, 'text': 'others', 'pos': 'NOUN', 'label': 'Entity', 'type': 'Group'}, {'token_id': 14, 'text': 'to', 'pos': 'PART', 'label': '', 'type': ''}, {'token_id': 'c022', 'text': 'take action', 'pos': 'VERB', 'label': 'Action', 'type': '', 'source_tokens': [15, 16]}, {'token_id': 17, 'text': 'and', 'pos': 'CCONJ', 'label': '', 'type': ''}, {'token_id': 18, 'text': 'not', 'pos': 'PART', 'label': '', 'type': ''}, {'token_id': 'c021', 'text': 'hope lose', 'pos': 'VERB', 'label': 'Action', 'type': '', 'source_tokens': [20, 19]}], 'ACTIONS': [{'act_id': 'a001', 'text': 'delivers', 'token_id': [3], 'ATTRIBUTES': {'POS': 'VERB', 'ENTITY_TYPE': ''}}, {'act_id': 'a002', 'text': 'moves', 'token_id': [11], 'ATTRIBUTES': {'POS': 'VERB', 'ENTITY_TYPE': ''}}, {'act_id': 'a004', 'text': 'take action', 'token_id': ['c022'], 'ATTRIBUTES': {'POS': 'VERB', 'ENTITY_TYPE': ''}}, {'act_id': 'a004', 'text': 'hope lose', 'token_id': ['c021'], 'ATTRIBUTES': {'POS': 'VERB', 'ENTITY_TYPE': ''}}], 'ENTITIES': [{'entity_id': 'e001', 'text': 'character', 'token_id': [2], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': 'Person', 'DET': {'text': 'a', 'token_id': [1]}}}, {'entity_id': 'e002', 'text': 'speech', 'token_id': [5], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': 'Object', 'ADJ': {'text': 'powerful', 'token_id': [7]}, 'DET': {'text': 'a', 'token_id': [4]}}}, {'entity_id': 'e003', 'text': 'others', 'token_id': [13], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': 'Group', 'DET': {'text': 'the', 'token_id': [12]}}}, {'entity_id': 'e004', 'text': 'action', 'token_id': [16], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': ''}}, {'entity_id': 'e005', 'text': 'hope', 'token_id': [20], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': ''}}], 'RELATIONS': [{'id': 'R005', 'relation_type': 'moves', 'tokens_info': [{'token_id': 5, 'text': 'speech', 'label': 'Entity', 'pos': 'NOUN', 'role': 'Agent'}, {'token_id': 11, 'text': 'moves', 'label': 'Action', 'pos': 'VERB', 'role': 'Action'}, {'token_id': 13, 'text': 'others', 'label': 'Entity', 'pos': 'NOUN', 'role': 'Recipient'}]}, {'id': 'R006', 'relation_type': 'leads to', 'tokens_info': [{'token_id': 11, 'text': 'moves', 'label': 'Action', 'pos': 'VERB'}, {'token_id': 'c022', 'text': 'take action', 'label': 'Action', 'pos': 'VERB'}]}, {'id': 'R007', 'relation_type': 'prevents', 'tokens_info': [{'token_id': 11, 'text': 'moves', 'label': 'Action', 'pos': 'VERB', 'role': 'Agent'}, {'token_id': 'c021', 'text': 'hope lose', 'label': 'Action', 'pos': 'VERB', 'role': 'Recipient'}]}], 'STATE_CHANGES': []}\n",
      "{'sentence_id': 'S003', 'sentence_name': 'None', 'original_sentence': \"An ugly (in personality or appearance)/overweight character who's in romantic pursuit of another who knows they feel that way but clearly doesn't reciprocate\", 'owner_id': 'ianchia', 'owner_username': 'ianchia', 'shared_with': [], 'tokens': [{'token_id': 0, 'text': 'An', 'pos': 'DET', 'label': '', 'type': ''}, {'token_id': 1, 'text': 'ugly', 'pos': 'ADJ', 'label': '', 'type': ''}, {'token_id': 2, 'text': '(', 'pos': 'PUNCT', 'label': '', 'type': ''}, {'token_id': 3, 'text': 'in', 'pos': 'ADP', 'label': '', 'type': ''}, {'token_id': 4, 'text': 'personality', 'pos': 'NOUN', 'label': 'Entity', 'type': ''}, {'token_id': 5, 'text': 'or', 'pos': 'CCONJ', 'label': '', 'type': ''}, {'token_id': 6, 'text': 'appearance)/overweight', 'pos': 'ADJ', 'label': '', 'type': ''}, {'token_id': 7, 'text': 'character', 'pos': 'NOUN', 'label': 'Entity', 'type': ''}, {'token_id': 8, 'text': 'who', 'pos': 'PRON', 'label': '', 'type': ''}, {'token_id': 9, 'text': \"'s\", 'pos': 'AUX', 'label': '', 'type': ''}, {'token_id': 10, 'text': 'in', 'pos': 'ADP', 'label': '', 'type': ''}, {'token_id': 11, 'text': 'romantic', 'pos': 'ADJ', 'label': '', 'type': ''}, {'token_id': 12, 'text': 'pursuit', 'pos': 'NOUN', 'label': 'Entity', 'type': ''}, {'token_id': 13, 'text': 'of', 'pos': 'ADP', 'label': '', 'type': ''}, {'token_id': 14, 'text': 'another', 'pos': 'PRON', 'label': '', 'type': ''}, {'token_id': 15, 'text': 'who', 'pos': 'PRON', 'label': '', 'type': ''}, {'token_id': 16, 'text': 'knows', 'pos': 'VERB', 'label': 'Action', 'type': ''}, {'token_id': 17, 'text': 'they', 'pos': 'PRON', 'label': '', 'type': ''}, {'token_id': 18, 'text': 'feel', 'pos': 'VERB', 'label': 'Action', 'type': ''}, {'token_id': 19, 'text': 'that', 'pos': 'DET', 'label': '', 'type': ''}, {'token_id': 20, 'text': 'way', 'pos': 'NOUN', 'label': 'Entity', 'type': ''}, {'token_id': 21, 'text': 'but', 'pos': 'CCONJ', 'label': '', 'type': ''}, {'token_id': 22, 'text': 'clearly', 'pos': 'ADV', 'label': '', 'type': ''}, {'token_id': 23, 'text': 'does', 'pos': 'AUX', 'label': '', 'type': ''}, {'token_id': 24, 'text': \"n't\", 'pos': 'PART', 'label': '', 'type': ''}, {'token_id': 25, 'text': 'reciprocate', 'pos': 'VERB', 'label': 'Action', 'type': ''}], 'ACTIONS': [{'act_id': 'a001', 'text': 'knows', 'token_id': [16], 'ATTRIBUTES': {'POS': 'VERB', 'ENTITY_TYPE': ''}}, {'act_id': 'a002', 'text': 'feel', 'token_id': [18], 'ATTRIBUTES': {'POS': 'VERB', 'ENTITY_TYPE': ''}}, {'act_id': 'a003', 'text': 'reciprocate', 'token_id': [25], 'ATTRIBUTES': {'POS': 'VERB', 'ENTITY_TYPE': ''}}], 'ENTITIES': [{'entity_id': 'e001', 'text': 'personality', 'token_id': [4], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': ''}}, {'entity_id': 'e002', 'text': 'character', 'token_id': [7], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': '', 'ADJ': {'text': 'appearance)/overweight', 'token_id': [6]}}}, {'entity_id': 'e003', 'text': 'pursuit', 'token_id': [12], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': '', 'ADJ': {'text': 'romantic', 'token_id': [11]}}}, {'entity_id': 'e004', 'text': 'way', 'token_id': [20], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': '', 'DET': {'text': 'that', 'token_id': [19]}}}]}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "coll = client[DB_NAME][COLL_NAME]\n",
    "\n",
    "count = coll.count_documents({})\n",
    "print(\"Total docs:\", count)\n",
    "\n",
    "for d in coll.find({}, {\"_id\": 0}).limit(3):\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe903b",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "- I have successfully connected to mongoDB\n",
    "\n",
    "- I have printed a few of the document examples and I am not able to see the internal structure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525c1eb",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158bd2a",
   "metadata": {},
   "source": [
    "### 4) Extract exactly what we need (text + target_json)      got error need debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622be97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 0\n",
      "No records!\n",
      "Wrote dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "import orjson\n",
    "\n",
    "def to_caseframe(doc):\n",
    "    # your text is stored in \"original_sentence\"\n",
    "    text = doc.get(\"original_sentence\", \"\")\n",
    "\n",
    "    # for now, weâ€™ll leave the slots empty;\n",
    "    # weâ€™ll fill ACTOR/ACTION/OBJECT later once we inspect the nested structure\n",
    "    frame = {\n",
    "        \"ACTOR\": \"\",\n",
    "        \"ACTION\": \"\",\n",
    "        \"OBJECT\": \"\",\n",
    "        \"LOCATION\": \"\",\n",
    "        \"TIME\": \"\"\n",
    "    }\n",
    "\n",
    "    return {\"text\": text.strip(), \"target_json\": frame}\n",
    "\n",
    "cursor = coll.find({}, {\"_id\": 0})\n",
    "records = [to_caseframe(d) for d in cursor if (d.get(\"text\") or d.get(\"sentence\"))]\n",
    "\n",
    "print(\"Loaded:\", len(records))\n",
    "print(records[0] if records else \"No records!\")\n",
    "\n",
    "# save to file (T5/BERT will both use this base file later)\n",
    "with open(\"dataset.jsonl\",\"wb\") as f:\n",
    "    for r in records:\n",
    "        f.write(orjson.dumps(r))\n",
    "        f.write(b\"\\n\")\n",
    "\n",
    "print(\"Wrote dataset.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667fd891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 327\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(MONGO_URI)\n",
    "coll = client[DB_NAME][COLL_NAME]\n",
    "\n",
    "print(\"Total docs:\", coll.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928d245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_sentence': 'A selfish, whiny teenaged girl'}\n",
      "['ACTIONS', 'ENTITIES', 'original_sentence', 'owner_id', 'owner_username', 'sentence_id', 'sentence_name', 'shared_with', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "# Show only the 'original_sentence' field for one doc\n",
    "print(coll.find_one({}, {\"_id\":0, \"original_sentence\":1}))\n",
    "\n",
    "# Show all top-level keys to confirm the exact spelling\n",
    "doc = coll.find_one({}, {\"_id\":0})\n",
    "print(sorted(list(doc.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6859cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs with original_sentence string: 327\n"
     ]
    }
   ],
   "source": [
    "has_original = coll.count_documents({\"original_sentence\": {\"$exists\": True, \"$type\": \"string\"}})\n",
    "print(\"Docs with original_sentence string:\", has_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d014c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded records: 327 | Missing text: 0\n",
      "{'text': 'A selfish, whiny teenaged girl', 'target_json': {'ACTOR': '', 'ACTION': '', 'OBJECT': '', 'LOCATION': '', 'TIME': ''}}\n",
      "Wrote dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "import orjson\n",
    "\n",
    "def to_caseframe(doc):\n",
    "    # try multiple likely keys just in case\n",
    "    text = (\n",
    "        doc.get(\"original_sentence\")\n",
    "        or doc.get(\"sentence\")\n",
    "        or doc.get(\"text\")\n",
    "        or \"\"\n",
    "    )\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text) if text is not None else \"\"\n",
    "    frame = {\"ACTOR\":\"\", \"ACTION\":\"\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\"}\n",
    "    return {\"text\": text.strip(), \"target_json\": frame}\n",
    "\n",
    "cursor = coll.find({}, {\"_id\": 0})\n",
    "records = []\n",
    "missing = 0\n",
    "\n",
    "for d in cursor:\n",
    "    r = to_caseframe(d)\n",
    "    if r[\"text\"]:\n",
    "        records.append(r)\n",
    "    else:\n",
    "        missing += 1\n",
    "\n",
    "print(\"Loaded records:\", len(records), \"| Missing text:\", missing)\n",
    "if records:\n",
    "    print(records[0])\n",
    "\n",
    "with open(\"dataset.jsonl\",\"wb\") as f:\n",
    "    for r in records:\n",
    "        f.write(orjson.dumps(r))\n",
    "        f.write(b\"\\n\")\n",
    "\n",
    "print(\"Wrote dataset.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6c907",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48c68a",
   "metadata": {},
   "source": [
    "### 5) Peek at one doc so we know what to extract next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c499e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "A selfish, whiny teenaged girl\n",
      "\n",
      "ACTIONS (first 2 shown):\n",
      "[]\n",
      "\n",
      "ENTITIES (first 3 shown):\n",
      "[{'entity_id': 'e001', 'text': 'girl', 'token_id': [5], 'ATTRIBUTES': {'POS': 'NOUN', 'ENTITY_TYPE': '', 'ADJ': {'text': 'teenaged', 'token_id': [4]}}}]\n",
      "\n",
      "RELATIONS (first 3 shown):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 â€” Inspect one document's structure (so we know how to pull slots)\n",
    "sample = coll.find_one({}, {\"_id\": 0})\n",
    "\n",
    "print(\"TEXT:\")\n",
    "print(sample.get(\"original_sentence\", \"\"))\n",
    "\n",
    "print(\"\\nACTIONS (first 2 shown):\")\n",
    "print(sample.get(\"ACTIONS\", [])[:2])\n",
    "\n",
    "print(\"\\nENTITIES (first 3 shown):\")\n",
    "print(sample.get(\"ENTITIES\", [])[:3])\n",
    "\n",
    "print(\"\\nRELATIONS (first 3 shown):\")\n",
    "print(sample.get(\"RELATIONS\", [])[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257bfb",
   "metadata": {},
   "source": [
    "### 6) Peek at tokens (so we know their structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dc29dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token sample (first 12):\n",
      "{'token_id': 0, 'text': 'A', 'pos': 'DET', 'label': '', 'type': ''}\n",
      "{'token_id': 1, 'text': 'selfish', 'pos': 'ADJ', 'label': '', 'type': ''}\n",
      "{'token_id': 2, 'text': ',', 'pos': 'PUNCT', 'label': '', 'type': ''}\n",
      "{'token_id': 3, 'text': 'whiny', 'pos': 'ADJ', 'label': '', 'type': ''}\n",
      "{'token_id': 4, 'text': 'teenaged', 'pos': 'ADJ', 'label': '', 'type': ''}\n",
      "{'token_id': 5, 'text': 'girl', 'pos': 'NOUN', 'label': 'Entity', 'type': ''}\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 â€” Inspect token structure\n",
    "tok = sample.get(\"tokens\", [])\n",
    "\n",
    "print(\"Token sample (first 12):\")\n",
    "if tok and isinstance(tok[0], dict):\n",
    "    # likely a list of dicts, e.g. {\"text\": \"...\", \"POS\": \"VERB\"}\n",
    "    for t in tok[:12]:\n",
    "        print(t)\n",
    "elif tok and isinstance(tok[0], str):\n",
    "    # plain list of strings\n",
    "    print(tok[:12])\n",
    "else:\n",
    "    print(tok)  # empty or unknown structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fae6d6",
   "metadata": {},
   "source": [
    "### 7) Pick ACTION from tokens (first VERB/AUX if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c9fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION: <none>\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 â€” Derive ACTION from tokens (first VERB/AUX if present)\n",
    "def pick_action_from_tokens(token_list):\n",
    "    if not token_list:\n",
    "        return \"\"\n",
    "    for t in token_list:\n",
    "        p = (t.get(\"pos\") or t.get(\"POS\") or \"\").upper()\n",
    "        if p in (\"VERB\", \"AUX\"):\n",
    "            return (t.get(\"text\") or \"\").strip()\n",
    "    # no verb found â†’ leave empty (this is okay for noun-phrase sentences)\n",
    "    return \"\"\n",
    "\n",
    "action_from_tokens = pick_action_from_tokens(sample.get(\"tokens\", []))\n",
    "print(\"ACTION:\", action_from_tokens if action_from_tokens else \"<none>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17f83c",
   "metadata": {},
   "source": [
    "### 8) pick ACTOR from ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c560ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR: girl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 â€” Derive ACTOR (first entity text)\n",
    "entities = sample.get(\"ENTITIES\", [])\n",
    "actor_text = \"\"\n",
    "\n",
    "if entities:\n",
    "    # take the first entity's text\n",
    "    actor_text = (entities[0].get(\"text\") or \"\").strip()\n",
    "\n",
    "print(\"ACTOR:\", actor_text if actor_text else \"<none>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e25b2",
   "metadata": {},
   "source": [
    "### 9) fill OBJECT (and leave LOCATION/TIME blank for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c8d5340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR: girl\n",
      "OBJECT: <none>\n",
      "LOCATION: <none>\n",
      "TIME: <none>\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 â€” Derive OBJECT, keep LOCATION/TIME empty if not annotated\n",
    "\n",
    "entities = sample.get(\"ENTITIES\", [])\n",
    "\n",
    "# We already picked ACTOR in Cell 8:\n",
    "actor_text = (entities[0].get(\"text\") or \"\").strip() if entities else \"\"\n",
    "\n",
    "# OBJECT: take the next entity text if present; else empty\n",
    "object_text = (entities[1].get(\"text\") or \"\").strip() if len(entities) > 1 else \"\"\n",
    "\n",
    "# LOCATION/TIME: if your data doesn't annotate these, leave them blank\n",
    "location_text = \"\"\n",
    "time_text = \"\"\n",
    "\n",
    "print(\"ACTOR:\", actor_text if actor_text else \"<none>\")\n",
    "print(\"OBJECT:\", object_text if object_text else \"<none>\")\n",
    "print(\"LOCATION:\", \"<none>\" if not location_text else location_text)\n",
    "print(\"TIME:\", \"<none>\" if not time_text else time_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69a00d",
   "metadata": {},
   "source": [
    "### 10) pick ACTOR / OBJECT by their ENTITY_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cfeb190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR: girl\n",
      "OBJECT: <none>\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 â€” Derive ACTOR and OBJECT based on ENTITY_TYPE\n",
    "\n",
    "entities = sample.get(\"ENTITIES\", [])\n",
    "\n",
    "actor_text = \"\"\n",
    "object_text = \"\"\n",
    "\n",
    "for e in entities:\n",
    "    etype_raw = e.get(\"ATTRIBUTES\", {}).get(\"ENTITY_TYPE\") or e.get(\"ENTITY_TYPE\") or \"\"\n",
    "    etype = (etype_raw.get(\"text\") if isinstance(etype_raw, dict) else etype_raw).strip().lower()\n",
    "    etext = (e.get(\"text\") or \"\").strip()\n",
    "\n",
    "    # simple rule: persons or agents â†’ ACTOR ; objects/things â†’ OBJECT\n",
    "    if not actor_text and etype in (\"person\", \"agent\", \"organisation\", \"organization\", \"group\"):\n",
    "        actor_text = etext\n",
    "    elif not object_text and etype in (\"object\", \"thing\", \"item\", \"entity\"):\n",
    "        object_text = etext\n",
    "\n",
    "# if still empty, fall back to the first / second entity positions\n",
    "if not actor_text and len(entities) > 0:\n",
    "    actor_text = (entities[0].get(\"text\") or \"\").strip()\n",
    "if not object_text and len(entities) > 1:\n",
    "    object_text = (entities[1].get(\"text\") or \"\").strip()\n",
    "\n",
    "print(\"ACTOR:\", actor_text if actor_text else \"<none>\")\n",
    "print(\"OBJECT:\", object_text if object_text else \"<none>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335523aa",
   "metadata": {},
   "source": [
    "### 11) Combine into one JSON frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "225ec970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'A selfish, whiny teenaged girl', 'target_json': {'ACTOR': 'girl', 'ACTION': '', 'OBJECT': '', 'LOCATION': '', 'TIME': ''}}\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 â€” Combine into one structured case-frame JSON for this sample\n",
    "\n",
    "frame = {\n",
    "    \"ACTOR\": actor_text,\n",
    "    \"ACTION\": action_from_tokens,\n",
    "    \"OBJECT\": object_text,\n",
    "    \"LOCATION\": \"\",\n",
    "    \"TIME\": \"\"\n",
    "}\n",
    "\n",
    "case_frame = {\"text\": sample.get(\"original_sentence\", \"\"), \"target_json\": frame}\n",
    "print(case_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036d3ce",
   "metadata": {},
   "source": [
    "### 12A) quick dataset audit (what do we have?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "344103fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 327\n",
      "Docs with ACTIONS list (non-empty): 262\n",
      "Docs with at least one VERB/AUX token: 270\n",
      "\n",
      "Top 10 ENTITY_TYPE values:\n",
      "  <empty>: 654\n",
      "  person: 93\n",
      "  abstract: 26\n",
      "  org: 15\n",
      "  entity: 13\n",
      "  object: 8\n",
      "  work_of_art: 4\n",
      "  place: 4\n",
      "  organization: 4\n",
      "  group: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 12A â€” Audit whatâ€™s annotated across all 327 docs\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "docs = list(coll.find({}, {\"_id\":0}))\n",
    "\n",
    "has_actions = sum(1 for d in docs if d.get(\"ACTIONS\"))\n",
    "has_tokens_verb = 0\n",
    "etype_counter = Counter()\n",
    "\n",
    "def first_verb(tokens):\n",
    "    for t in tokens or []:\n",
    "        p = (t.get(\"pos\") or t.get(\"POS\") or \"\").upper()\n",
    "        if p in (\"VERB\",\"AUX\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for d in docs:\n",
    "    if first_verb(d.get(\"tokens\", [])):\n",
    "        has_tokens_verb += 1\n",
    "    for e in d.get(\"ENTITIES\", []):\n",
    "        et_raw = e.get(\"ATTRIBUTES\", {}).get(\"ENTITY_TYPE\") or e.get(\"ENTITY_TYPE\") or \"\"\n",
    "        et = (et_raw.get(\"text\") if isinstance(et_raw, dict) else et_raw) or \"\"\n",
    "        etype_counter[et.strip().lower()] += 1\n",
    "\n",
    "print(f\"Total docs: {len(docs)}\")\n",
    "print(f\"Docs with ACTIONS list (non-empty): {has_actions}\")\n",
    "print(f\"Docs with at least one VERB/AUX token: {has_tokens_verb}\")\n",
    "print(\"\\nTop 10 ENTITY_TYPE values:\")\n",
    "for k, v in etype_counter.most_common(10):\n",
    "    print(f\"  {k or '<empty>'}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466cfd0d",
   "metadata": {},
   "source": [
    "### 13) final extractor (one function for all docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7c442f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 â€” build one function to extract case-frame from a document\n",
    "import orjson\n",
    "\n",
    "def extract_caseframe(doc):\n",
    "    \"\"\"Return one {'text':..., 'target_json': {...}} record from a Mongo document.\"\"\"\n",
    "    text = (doc.get(\"original_sentence\") or \"\").strip()\n",
    "\n",
    "    # ----- ACTION -----\n",
    "    actions = doc.get(\"ACTIONS\", [])\n",
    "    if actions:\n",
    "        action_text = (actions[0].get(\"text\") or \"\").strip()\n",
    "    else:\n",
    "        # fallback: pick first verb from tokens\n",
    "        action_text = \"\"\n",
    "        for t in doc.get(\"tokens\", []):\n",
    "            p = (t.get(\"pos\") or t.get(\"POS\") or \"\").upper()\n",
    "            if p in (\"VERB\", \"AUX\"):\n",
    "                action_text = (t.get(\"text\") or \"\").strip()\n",
    "                break\n",
    "\n",
    "    # ----- ENTITY TYPES -----\n",
    "    actor = object_ = location = time_ = \"\"\n",
    "    for e in doc.get(\"ENTITIES\", []):\n",
    "        et_raw = e.get(\"ATTRIBUTES\", {}).get(\"ENTITY_TYPE\") or e.get(\"ENTITY_TYPE\") or \"\"\n",
    "        etype = (et_raw.get(\"text\") if isinstance(et_raw, dict) else et_raw).strip().lower()\n",
    "        etext = (e.get(\"text\") or \"\").strip()\n",
    "\n",
    "        if not actor and etype in (\"person\", \"agent\", \"org\", \"organization\", \"group\"):\n",
    "            actor = etext\n",
    "        elif not object_ and etype in (\"object\", \"thing\", \"entity\"):\n",
    "            object_ = etext\n",
    "        elif not location and etype in (\"place\", \"location\"):\n",
    "            location = etext\n",
    "        elif not time_ and etype in (\"time\", \"date\", \"event\"):\n",
    "            time_ = etext\n",
    "\n",
    "    # fallback positions if still empty\n",
    "    ents = doc.get(\"ENTITIES\", [])\n",
    "    if not actor and len(ents) > 0:\n",
    "        actor = (ents[0].get(\"text\") or \"\").strip()\n",
    "    if not object_ and len(ents) > 1:\n",
    "        object_ = (ents[1].get(\"text\") or \"\").strip()\n",
    "\n",
    "    frame = {\n",
    "        \"ACTOR\": actor,\n",
    "        \"ACTION\": action_text,\n",
    "        \"OBJECT\": object_,\n",
    "        \"LOCATION\": location,\n",
    "        \"TIME\": time_\n",
    "    }\n",
    "    return {\"text\": text, \"target_json\": frame}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589459ff",
   "metadata": {},
   "source": [
    "### 14) apply to all docs and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79f1b7f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted: 327 records\n",
      "\n",
      "{\n",
      "  \"text\": \"A selfish, whiny teenaged girl\",\n",
      "  \"target_json\": {\n",
      "    \"ACTOR\": \"girl\",\n",
      "    \"ACTION\": \"\",\n",
      "    \"OBJECT\": \"\",\n",
      "    \"LOCATION\": \"\",\n",
      "    \"TIME\": \"\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "{\n",
      "  \"text\": \"When a character delivers a speech so powerful that it emotionally moves the others to take action and not lose hope\",\n",
      "  \"target_json\": {\n",
      "    \"ACTOR\": \"character\",\n",
      "    \"ACTION\": \"delivers\",\n",
      "    \"OBJECT\": \"speech\",\n",
      "    \"LOCATION\": \"\",\n",
      "    \"TIME\": \"\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "{\n",
      "  \"text\": \"An ugly (in personality or appearance)/overweight character who's in romantic pursuit of another who knows they feel that way but clearly doesn't reciprocate\",\n",
      "  \"target_json\": {\n",
      "    \"ACTOR\": \"personality\",\n",
      "    \"ACTION\": \"knows\",\n",
      "    \"OBJECT\": \"character\",\n",
      "    \"LOCATION\": \"\",\n",
      "    \"TIME\": \"\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "{\n",
      "  \"text\": \"sÃ¡Â»Â©c khoÃ¡ÂºÂ» tÃ¡Â»â€¢ng Ã„â€˜ÃƒÂ n Ã¡Â»â€¢n Ã„â€˜Ã¡Â»â€¹nh, tÃ¡Â»â€° lÃ¡Â»â€¡ hÃƒÂ´ hÃ¡ÂºÂ¥p thÃ¡ÂºÂ¥p, hÃ¡ÂºÂ¿t cÃƒÂ¡m sÃ¡Â»â€ºm.\",\n",
      "  \"target_json\": {\n",
      "    \"ACTOR\": \"Ã„â€˜Ãƒ\",\n",
      "    \"ACTION\": \"hÃ¡ÂºÂ¥p\",\n",
      "    \"OBJECT\": \"c khoÃ¡Âº\",\n",
      "    \"LOCATION\": \"\",\n",
      "    \"TIME\": \"\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "{\n",
      "  \"text\": \"A movie adaptation of a novel\",\n",
      "  \"target_json\": {\n",
      "    \"ACTOR\": \"novel\",\n",
      "    \"ACTION\": \"\",\n",
      "    \"OBJECT\": \"novel\",\n",
      "    \"LOCATION\": \"\",\n",
      "    \"TIME\": \"\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ’¾ Wrote all records to dataset_full.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 14 â€” Apply extractor to all documents and save\n",
    "\n",
    "cursor = coll.find({}, {\"_id\": 0})\n",
    "records = [extract_caseframe(doc) for doc in cursor]\n",
    "\n",
    "print(\"âœ… Extracted:\", len(records), \"records\\n\")\n",
    "\n",
    "# Preview a few samples\n",
    "for r in records[:5]:\n",
    "    print(orjson.dumps(r, option=orjson.OPT_INDENT_2).decode(\"utf-8\"))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Save to JSONL file for later model training\n",
    "with open(\"dataset_full.jsonl\", \"wb\") as f:\n",
    "    for r in records:\n",
    "        f.write(orjson.dumps(r))\n",
    "        f.write(b\"\\n\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Wrote all records to dataset_full.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6851a",
   "metadata": {},
   "source": [
    "### 15) Load the dataset you just exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eddd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records loaded: 327\n",
      "Example:\n",
      "{\n",
      "  \"text\": \"A selfish, whiny teenaged girl\",\n",
      "  \"target_json\": {\n",
      "    \"ACTOR\": \"girl\",\n",
      "    \"ACTION\": \"\",\n",
      "    \"OBJECT\": \"\",\n",
      "    \"LOCATION\": \"\",\n",
      "    \"TIME\": \"\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 15 â€” Load the exported dataset\n",
    "import json\n",
    "\n",
    "records = []\n",
    "with open(\"dataset_full.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        records.append(json.loads(line))\n",
    "\n",
    "print(\"Total records loaded:\", len(records))\n",
    "print(\"Example:\")\n",
    "print(json.dumps(records[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa610417",
   "metadata": {},
   "source": [
    "### 16)  Split into train / validation / test\n",
    "\n",
    "Weâ€™ll keep ~80% for training, 10% for validation, 10% for testing.\n",
    "\n",
    "Weâ€™ll also shuffle first so the order doesnâ€™t matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c5bad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 261 | Validation: 33 | Test: 33\n"
     ]
    }
   ],
   "source": [
    "# Cell 16 â€” Split dataset into train/val/test sets\n",
    "import random, orjson\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(records)\n",
    "\n",
    "n = len(records)\n",
    "train_split = int(0.8 * n)\n",
    "val_split = int(0.9 * n)\n",
    "\n",
    "train_records = records[:train_split]\n",
    "val_records = records[train_split:val_split]\n",
    "test_records = records[val_split:]\n",
    "\n",
    "print(f\"Train: {len(train_records)} | Validation: {len(val_records)} | Test: {len(test_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf7dc5",
   "metadata": {},
   "source": [
    "### 17) Save the splits to separate JSONL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83f3a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved train.jsonl, val.jsonl, and test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 17 â€” Save the split datasets\n",
    "def save_jsonl(filename, data):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for r in data:\n",
    "            f.write(orjson.dumps(r))\n",
    "            f.write(b\"\\n\")\n",
    "\n",
    "save_jsonl(\"train.jsonl\", train_records)\n",
    "save_jsonl(\"val.jsonl\", val_records)\n",
    "save_jsonl(\"test.jsonl\", test_records)\n",
    "\n",
    "print(\" Saved train.jsonl, val.jsonl, and test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcefcd",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0334db",
   "metadata": {},
   "source": [
    "## Training Time\n",
    "\n",
    "Goal: fine-tune a BERT model to label each token as part of a slot (ACTOR, ACTION, OBJECT, LOCATION, TIME).\n",
    "\n",
    "Weâ€™ll do it in these small stages:\n",
    "\n",
    "**Stage**\t  |    **Purpose**\n",
    "\n",
    "Cell 18\t  |    install libraries\n",
    "\n",
    "Cell 19\t  |    prepare data â†’ tokens + labels (BIO format)\n",
    "\n",
    "Cell 20\t  |     tokenize & align labels for BERT\n",
    "\n",
    "Cell 21\t  |    train BERT\n",
    "\n",
    "Cell 22\t  |    evaluate on test set (Slot-F1 & Frame-Validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e486eb",
   "metadata": {},
   "source": [
    "### Cell 18 â€” install and import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd6731a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18A â€” Install missing 'evaluate' library\n",
    "!pip install evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4522618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 18 â€” Install and import libraries for BERT slot tagging\n",
    "!pip install transformers datasets seqeval --quiet\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate, numpy as np, json, orjson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbaafd6",
   "metadata": {},
   "source": [
    "### Cell 19 â€” Convert your JSON records to token-label pairs (BIO format)\n",
    "\n",
    "For each record:\n",
    "\n",
    "split the sentence into tokens (simple whitespace for now),\n",
    "\n",
    "create one label per token (B-ACTOR, I-ACTOR, O, etc.) based on the words appearing in each slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb80af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'labels'],\n",
      "        num_rows: 261\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'labels'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'labels'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "})\n",
      "\n",
      "Example record:\n",
      "{'tokens': ['Fat-related', 'insults'], 'labels': ['O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "# Cell 19 â€” Build BIO tagging dataset for BERT\n",
    "import re\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    # very simple whitespace tokenizer\n",
    "    return text.split()\n",
    "\n",
    "def make_bio_labels(text, frame):\n",
    "    tokens = simple_tokenize(text)\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for slot in [\"ACTOR\",\"ACTION\",\"OBJECT\",\"LOCATION\",\"TIME\"]:\n",
    "        phrase = frame.get(slot, \"\").strip()\n",
    "        if not phrase:\n",
    "            continue\n",
    "        phrase_tokens = phrase.split()\n",
    "        # find matching subsequence of tokens in text\n",
    "        for i in range(len(tokens)):\n",
    "            window = tokens[i:i+len(phrase_tokens)]\n",
    "            if [t.lower() for t in window] == [p.lower() for p in phrase_tokens]:\n",
    "                labels[i] = f\"B-{slot}\"\n",
    "                for j in range(1, len(phrase_tokens)):\n",
    "                    labels[i+j] = f\"I-{slot}\"\n",
    "                break\n",
    "    return tokens, labels\n",
    "\n",
    "def build_dataset_from_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            text = r[\"text\"]\n",
    "            frame = r[\"target_json\"]\n",
    "            toks, labs = make_bio_labels(text, frame)\n",
    "            data.append({\"tokens\": toks, \"labels\": labs})\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "train_ds = build_dataset_from_jsonl(\"train.jsonl\")\n",
    "val_ds   = build_dataset_from_jsonl(\"val.jsonl\")\n",
    "test_ds  = build_dataset_from_jsonl(\"test.jsonl\")\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "print(datasets)\n",
    "print(\"\\nExample record:\")\n",
    "print(datasets[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943124d7",
   "metadata": {},
   "source": [
    "### Cell 20 â€” Tokenize and align labels for BERT\n",
    "\n",
    "BERT uses its own tokenizer (wordpiece).\n",
    "\n",
    "so we must align our word-level labels (like [\"O\",\"B-ACTOR\"]) to match BERTâ€™s subword tokens (for example, â€œteenagedâ€ â†’ â€œteenâ€, â€œ##agedâ€).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfb3a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261/261 [00:00<00:00, 1650.07 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 1373.45 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 1026.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'orig_tokens'],\n",
      "        num_rows: 261\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'orig_tokens'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'orig_tokens'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "})\n",
      "Train features: {'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'orig_tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 20 (fix) â€” Rebuild tokenized datasets and KEEP labels/tokens\n",
    "\n",
    "label_list = [\"O\",\"B-ACTOR\",\"I-ACTOR\",\"B-ACTION\",\"I-ACTION\",\n",
    "              \"B-OBJECT\",\"I-OBJECT\",\"B-LOCATION\",\"I-LOCATION\",\"B-TIME\",\"I-TIME\"]\n",
    "\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in id2label.items()}\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True, max_length=128)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    labels = []\n",
    "    for wid in word_ids:\n",
    "        if wid is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(label2id.get(example[\"labels\"][wid], 0))\n",
    "    tokenized[\"labels\"] = labels\n",
    "    # keep original tokens around for later analysis\n",
    "    tokenized[\"orig_tokens\"] = example[\"tokens\"]\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align, batched=False)\n",
    "\n",
    "print(tokenized_datasets)\n",
    "print(\"Train features:\", tokenized_datasets[\"train\"].features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed28d0",
   "metadata": {},
   "source": [
    "### Cell 21 â€” Train BERT (lightweight on your dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d4f094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ian Chia\\.conda\\envs\\it3386\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "# Cell 21A â€” Upgrade transformers to the latest version\n",
    "!pip install -U transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4a2a2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.57.1\n",
      "Uninstalling transformers-4.57.1:\n",
      "  Successfully uninstalled transformers-4.57.1\n",
      "Found existing installation: tokenizers 0.22.1\n",
      "Uninstalling tokenizers-0.22.1:\n",
      "  Successfully uninstalled tokenizers-0.22.1\n",
      "Reinstall complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 21B â€” fully reinstall transformers + tokenizers (Windows-safe)\n",
    "import sys, os, shutil\n",
    "\n",
    "# 1) Uninstall first\n",
    "!python -m pip uninstall -y transformers tokenizers\n",
    "\n",
    "# 2) Try to remove any half-deleted tokenizers folders (Windows sometimes locks them)\n",
    "candidates = [\n",
    "    r\"C:\\Users\\ian Chia\\.conda\\envs\\it3386\\Lib\\site-packages\\tokenizers\",\n",
    "    r\"C:\\Users\\ian Chia\\.conda\\envs\\it3386\\Lib\\site-packages\\~okenizers\",\n",
    "]\n",
    "for d in candidates:\n",
    "    if os.path.exists(d):\n",
    "        try:\n",
    "            shutil.rmtree(d, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(\"Could not remove\", d, \"->\", e)\n",
    "\n",
    "# 3) Reinstall a known-good, compatible set\n",
    "!python -m pip install --no-cache-dir -q transformers==4.44.2 tokenizers==0.19.1 datasets==2.20.0 evaluate==0.4.2 accelerate==0.33.0\n",
    "print(\"Reinstall complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf0e2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\ian Chia\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--seqeval\\541ae017dc683f85116597d48f621abc7b21b88dc42ec937c71af5415f0af63c (last modified on Thu Nov  6 17:09:05 2025) since it couldn't be found locally at evaluate-metric--seqeval, or remotely on the Hugging Face Hub.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 10:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.006645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.377000</td>\n",
       "      <td>0.877424</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.729064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.868200</td>\n",
       "      <td>0.805984</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.780788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.727600</td>\n",
       "      <td>0.774504</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.592200</td>\n",
       "      <td>0.793113</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.464516</td>\n",
       "      <td>0.780788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.802860</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.773399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.820745</td>\n",
       "      <td>0.471910</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.775862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>0.854624</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.473373</td>\n",
       "      <td>0.773399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>0.844077</td>\n",
       "      <td>0.472527</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.497110</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>0.854331</td>\n",
       "      <td>0.460674</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.479532</td>\n",
       "      <td>0.775862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Cell 21 â€” Fine-tune BERT for slot tagging\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate, numpy as np\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # remove positions with -100 and map ids -> tag strings\n",
    "    true_tags, pred_tags = [], []\n",
    "    for p_seq, l_seq in zip(preds, labels):\n",
    "        t_seq, p2_seq = [], []\n",
    "        for p_id, l_id in zip(p_seq, l_seq):\n",
    "            if l_id == -100:\n",
    "                continue\n",
    "            t_seq.append(id2label[l_id])\n",
    "            p2_seq.append(id2label[p_id])\n",
    "        true_tags.append(t_seq)\n",
    "        pred_tags.append(p2_seq)\n",
    "\n",
    "    results = seqeval.compute(predictions=pred_tags, references=true_tags, scheme=\"IOB2\")\n",
    "    # report overall slot-F1 (macro)\n",
    "    return {\n",
    "        \"overall_precision\": results.get(\"overall_precision\", 0.0),\n",
    "        \"overall_recall\": results.get(\"overall_recall\", 0.0),\n",
    "        \"overall_f1\": results.get(\"overall_f1\", 0.0),\n",
    "        \"overall_accuracy\": results.get(\"overall_accuracy\", 0.0)\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_slot\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045dfee",
   "metadata": {},
   "source": [
    "### Cell 22 â€” Evaluate on the test split (Slot-F1 etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e14377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"eval_loss\": 0.7551255822181702,\n",
      "  \"eval_overall_precision\": 0.32558139534883723,\n",
      "  \"eval_overall_recall\": 0.37333333333333335,\n",
      "  \"eval_overall_f1\": 0.34782608695652173,\n",
      "  \"eval_overall_accuracy\": 0.7869565217391304,\n",
      "  \"eval_runtime\": 2.2882,\n",
      "  \"eval_samples_per_second\": 14.422,\n",
      "  \"eval_steps_per_second\": 1.311,\n",
      "  \"epoch\": 10.0\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Cell 22 â€” Test-set evaluation (uses the same compute_metrics)\n",
    "test_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "import json\n",
    "print(json.dumps(test_metrics, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39bbfc0",
   "metadata": {},
   "source": [
    "### Cell 23 â€” Turn BIO â†’ frames and compute Frame-Validity %\n",
    "\n",
    "Definition weâ€™ll use (suited to your annotations):\n",
    "\n",
    "A frame is â€œvalidâ€ if at least one of ACTOR / ACTION / OBJECT is non-empty (LOCATION/TIME are optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa091f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame-Validity %: 81.8%\n",
      "\n",
      "Text: Explains the standard terminology for different ways various works use tropes\n",
      "Pred: {'ACTOR': 'tropes', 'ACTION': 'use', 'OBJECT': 'ways', 'LOCATION': '', 'TIME': ''}\n",
      "Gold: {'ACTOR': 'ways', 'ACTION': 'Explains', 'OBJECT': 'works', 'LOCATION': '', 'TIME': ''}\n",
      "\n",
      "Text: Pigs with pneumonia\n",
      "Pred: {'ACTOR': 'Pigs', 'ACTION': '', 'OBJECT': '', 'LOCATION': '', 'TIME': ''}\n",
      "Gold: {'ACTOR': 'Pigs', 'ACTION': '', 'OBJECT': 'pneumonia', 'LOCATION': '', 'TIME': ''}\n",
      "\n",
      "Text: Flashbacks in a work are always black and white to represent a specific time period\n",
      "Pred: {'ACTOR': 'work', 'ACTION': '', 'OBJECT': '', 'LOCATION': '', 'TIME': ''}\n",
      "Gold: {'ACTOR': 'Flashbacks', 'ACTION': 'represent', 'OBJECT': 'work', 'LOCATION': '', 'TIME': ''}\n"
     ]
    }
   ],
   "source": [
    "# Cell 23 â€” BIO â†’ frame, Frame-Validity %, and a few examples\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def ids_to_tags(pred_ids, label_ids):\n",
    "    \"\"\"Remove -100 and map ids -> tag strings.\"\"\"\n",
    "    pred_tags, true_tags = [], []\n",
    "    for p_seq, l_seq in zip(pred_ids, label_ids):\n",
    "        seq_p, seq_t = [], []\n",
    "        for p_id, l_id in zip(p_seq, l_seq):\n",
    "            if l_id == -100:\n",
    "                continue\n",
    "            seq_p.append(id2label[p_id])\n",
    "            seq_t.append(id2label[l_id])\n",
    "        pred_tags.append(seq_p); true_tags.append(seq_t)\n",
    "    return pred_tags, true_tags\n",
    "\n",
    "def bio_to_frame(tokens, tags):\n",
    "    slots = {k:\"\" for k in [\"ACTOR\",\"ACTION\",\"OBJECT\",\"LOCATION\",\"TIME\"]}\n",
    "    cur_slot, buf = None, []\n",
    "    for tok, tag in zip(tokens, tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if cur_slot and buf:\n",
    "                slots[cur_slot] = \" \".join(buf)\n",
    "            cur_slot, buf = tag[2:], [tok]\n",
    "        elif tag.startswith(\"I-\") and cur_slot == tag[2:]:\n",
    "            buf.append(tok)\n",
    "        else:\n",
    "            if cur_slot and buf:\n",
    "                slots[cur_slot] = \" \".join(buf)\n",
    "            cur_slot, buf = None, []\n",
    "    if cur_slot and buf:\n",
    "        slots[cur_slot] = \" \".join(buf)\n",
    "    return slots\n",
    "\n",
    "def frame_valid(slots):\n",
    "    # Valid if at least one of the core slots is non-empty\n",
    "    return any(slots[k].strip() for k in [\"ACTOR\",\"ACTION\",\"OBJECT\"])\n",
    "\n",
    "# 1) get predictions\n",
    "pred = trainer.predict(tokenized_datasets[\"test\"])\n",
    "pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "true_ids = pred.label_ids\n",
    "\n",
    "# 2) ids â†’ tag strings (strip -100)\n",
    "pred_tags, true_tags = ids_to_tags(pred_ids, true_ids)\n",
    "\n",
    "# 3) rebuild tokens to convert BIO â†’ frames\n",
    "tokens_list = tokenized_datasets[\"test\"][\"orig_tokens\"]\n",
    "\n",
    "# 4) compute Frame-Validity %\n",
    "valid_count = 0\n",
    "frames_pred, frames_true = [], []\n",
    "for toks, p_tags, t_tags in zip(tokens_list, pred_tags, true_tags):\n",
    "    p_frame = bio_to_frame(toks, p_tags)\n",
    "    t_frame = bio_to_frame(toks, t_tags)\n",
    "    frames_pred.append(p_frame); frames_true.append(t_frame)\n",
    "    if frame_valid(p_frame):\n",
    "        valid_count += 1\n",
    "\n",
    "frame_validity = valid_count / len(tokens_list)\n",
    "print(f\"Frame-Validity %: {frame_validity*100:.1f}%\")\n",
    "\n",
    "# 5) show a few examples (pred vs gold)\n",
    "for i in range(3):\n",
    "    print(\"\\nText:\", \" \".join(tokens_list[i]))\n",
    "    print(\"Pred:\", frames_pred[i])\n",
    "    print(\"Gold:\", frames_true[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6678b9",
   "metadata": {},
   "source": [
    "## Result Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66c850",
   "metadata": {},
   "source": [
    "**Slot-F1 = 0.35**\n",
    "\n",
    "This metric tells you how accurately each slot (ACTOR, ACTION, OBJECT, etc.) was tagged at the token level.\n",
    "\n",
    "0.35 means the model is identifying some slots correctly, but itâ€™s also making quite a few mistakes.\n",
    "\n",
    "This is normal and even expected for your current dataset because:\n",
    "\n",
    "your annotations mostly label entities but not specific roles,\n",
    "\n",
    "ACTION slots are sparse (many sentences have no verbs tagged),\n",
    "\n",
    "the dataset is small (327 sentences total).\n",
    "\n",
    "So the model is learning some patterns but not yet strong generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08b19b",
   "metadata": {},
   "source": [
    "**Frame-Validity = 81.8%**\n",
    "\n",
    "This measures: how often the model outputs a structured, meaningful frame (i.e., at least one of ACTOR, ACTION, or OBJECT is filled in).\n",
    "\n",
    "81.8% means that in ~4 out of 5 sentences, the model generated a valid frame instead of an empty or nonsense output.\n",
    "\n",
    "This is quite good for such a small and noisy dataset.\n",
    "\n",
    "It means your pipeline (data â†’ model â†’ JSON reconstruction) is working correctly, and the model is producing coherent results most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946284c",
   "metadata": {},
   "source": [
    "**Overall Accuracy = 79%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516646ce",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The BERT model was trained on 327 annotated sentences.\n",
    "It achieved a Slot-F1 of 0.35, indicating partial learning of role boundaries, and a Frame-Validity of 81.8%, showing that most outputs followed a valid frame structure.\n",
    "These results suggest that the current annotation coverage provides a solid proof-of-concept pipeline but requires finer role distinctions (especially for ACTION vs ENTITY types) to improve accuracy.\n",
    "The system demonstrates feasibility for automated text-to-frame conversion and provides a baseline for comparison with the T5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316885a",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3005cfb",
   "metadata": {},
   "source": [
    "# T5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4889e",
   "metadata": {},
   "source": [
    "### Cell T1 â€” imports for T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd9f120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import json, orjson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4115a",
   "metadata": {},
   "source": [
    "###  Cell T2 â€” load your splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67169523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 261 examples [00:00, 5192.92 examples/s]\n",
      "Generating train split: 33 examples [00:00, 1245.84 examples/s]\n",
      "Generating train split: 33 examples [00:00, 1609.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Fat-related insults', 'target_json': {'ACTOR': 'Fat - related insults', 'ACTION': 'related', 'OBJECT': '', 'LOCATION': '', 'TIME': ''}}\n",
      "261 33 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell T2 â€” load train/val/test (same files you saved earlier)\n",
    "train_ds = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\n",
    "val_ds   = load_dataset(\"json\", data_files=\"val.jsonl\",   split=\"train\")\n",
    "test_ds  = load_dataset(\"json\", data_files=\"test.jsonl\",  split=\"train\")\n",
    "\n",
    "print(train_ds[0])\n",
    "print(len(train_ds), len(val_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7ef40",
   "metadata": {},
   "source": [
    "### Cell T3 â€” build input/target strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee38a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261/261 [00:00<00:00, 2668.82 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 1014.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 851.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract slots: Fat-related insults\n",
      "{\"ACTOR\":\"Fat - related insults\",\"ACTION\":\"related\",\"OBJECT\":\"\",\"LOCATION\":\"\",\"TIME\":\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell T3 â€” map each row to input_text and target_text\n",
    "def to_io(example):\n",
    "    tgt = example[\"target_json\"]\n",
    "    example[\"input_text\"]  = f\"extract slots: {example['text']}\"\n",
    "    example[\"target_text\"] = json.dumps({\n",
    "        \"ACTOR\":   tgt.get(\"ACTOR\",\"\"),\n",
    "        \"ACTION\":  tgt.get(\"ACTION\",\"\"),\n",
    "        \"OBJECT\":  tgt.get(\"OBJECT\",\"\"),\n",
    "        \"LOCATION\":tgt.get(\"LOCATION\",\"\"),\n",
    "        \"TIME\":    tgt.get(\"TIME\",\"\")\n",
    "    }, ensure_ascii=False, separators=(\",\",\":\"))\n",
    "    return example\n",
    "\n",
    "train_io = train_ds.map(to_io)\n",
    "val_io   = val_ds.map(to_io)\n",
    "test_io  = test_ds.map(to_io)\n",
    "\n",
    "print(train_io[0][\"input_text\"])\n",
    "print(train_io[0][\"target_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea39472",
   "metadata": {},
   "source": [
    "### Cell T4 â€” tokenizer + model + tokenization fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ee526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ian Chia\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Map:   0%|                                                            | 0/261 [00:00<?, ? examples/s]C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261/261 [00:00<00:00, 1944.49 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 505.56 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 590.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    # encode input\n",
    "    model_inputs = tok(batch[\"input_text\"], truncation=True, max_length=256)\n",
    "    # encode target\n",
    "    with tok.as_target_tokenizer():\n",
    "        labels = tok(batch[\"target_text\"], truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_io.map(tok_fn, batched=True, remove_columns=train_io.column_names)\n",
    "val_tok   = val_io.map(tok_fn,   batched=True, remove_columns=val_io.column_names)\n",
    "test_tok  = test_io.map(tok_fn,  batched=True, remove_columns=test_io.column_names)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tok, model=model)\n",
    "\n",
    "print(train_tok[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb649ac",
   "metadata": {},
   "source": [
    "### Cell T5 â€” training args + trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77cf52d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell T5 â€” training setup\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_slots\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Trainer ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b2d06",
   "metadata": {},
   "source": [
    "### Cell T6 â€” train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54340750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 24:03, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.587821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.895100</td>\n",
       "      <td>0.396432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.561700</td>\n",
       "      <td>0.376771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.339412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.331424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.335627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.342082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.311399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.337047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.343844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.339023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.336311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.099200</td>\n",
       "      <td>0.343640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.099200</td>\n",
       "      <td>0.347123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.091400</td>\n",
       "      <td>0.346782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Cell T6 â€” fine-tune T5\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a32c9",
   "metadata": {},
   "source": [
    "### Cell T7 â€” evaluate on test (generate â†’ metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca777704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot-F1: 0.374\n",
      "Frame-Validity %: 57.6%\n",
      "\n",
      "Text: Explains the standard terminology for different ways various works use tropes\n",
      "Pred: \"ACTOR\":\"standard terminology\",\"ACTION\":\"Explics\",\"OBJECT\":\"variable ways\",\"LOCATION\":\"\",\"TIME\":\"\"\n",
      "Gold: {\"ACTOR\":\"ways\",\"ACTION\":\"Explains\",\"OBJECT\":\"works\",\"LOCATION\":\"\",\"TIME\":\"\"}\n",
      "\n",
      "Text: Pigs with pneumonia\n",
      "Pred: \"ACTOR\":\"Pigs\",\"ACTION\":\"\",\"OBJECT\":\" pneumonia\",\"LOCATION\":\"\",\"TIME\":\"\"\n",
      "Gold: {\"ACTOR\":\"Pigs\",\"ACTION\":\"\",\"OBJECT\":\"pneumonia\",\"LOCATION\":\"\",\"TIME\":\"\"}\n",
      "\n",
      "Text: Flashbacks in a work are always black and white to represent a specific time period\n",
      "Pred: \"ACTOR\":\"work\",\"ACTION\":\"represent\",\"OBJECT\":\"black and white\",\"LOCATION\":\"\",\"TIME\":\"\"\n",
      "Gold: {\"ACTOR\":\"Flashbacks\",\"ACTION\":\"represent\",\"OBJECT\":\"work\",\"LOCATION\":\"\",\"TIME\":\"\"}\n",
      "\n",
      "Text: The herd's health is good, so their feed intake exceeds the standard.\n",
      "Pred: \"ACTOR\":\"herd's health\",\"ACTION\":\"exceeds\",\"OBJECT\":\"feed intake\",\"LOCATION\":\"\",\"TIME\":\"\"\n",
      "Gold: {\"ACTOR\":\"standard\",\"ACTION\":\"exceeds\",\"OBJECT\":\"herd 's health\",\"LOCATION\":\"\",\"TIME\":\"\"}\n",
      "\n",
      "Text: Eats loads and loads of food\n",
      "Pred: \"ACTOR\":\"eats\",\"ACTION\":\"eats\",\"OBJECT\":\"loads\",\"LOCATION\":\"\",\"TIME\":\"\"\n",
      "Gold: {\"ACTOR\":\"loads\",\"ACTION\":\"Eats\",\"OBJECT\":\"loads\",\"LOCATION\":\"\",\"TIME\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "# Cell T7 â€” Evaluate T5 on test set (robust JSON + metrics)\n",
    "import torch, json, re\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "# ----- schema -----\n",
    "class Frame(BaseModel):\n",
    "    ACTOR: str\n",
    "    ACTION: str\n",
    "    OBJECT: str\n",
    "    LOCATION: str\n",
    "    TIME: str\n",
    "\n",
    "# ----- helpers -----\n",
    "def norm(x):\n",
    "    return re.sub(r\"\\s+\", \" \", (x or \"\")).strip().lower()\n",
    "\n",
    "def safe_json(s):\n",
    "    s = (s or \"\").strip().replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"<s>\", \"\").strip()\n",
    "    # wrap loose key/value pairs (no braces) like \"ACTOR\":\"x\",\"ACTION\":\"y\",...\n",
    "    if s and not s.lstrip().startswith(\"{\") and \"\\\"ACTOR\\\"\" in s:\n",
    "        s = \"{\" + s + \"}\"\n",
    "    # normalize smart quotes, remove trailing commas before } or ]\n",
    "    s = s.replace(\"â€œ\", \"\\\"\").replace(\"â€\", \"\\\"\").replace(\"â€™\", \"'\")\n",
    "    s = re.sub(r\",\\s*([}\\]])\", r\"\\1\", s)\n",
    "    try:\n",
    "        return json.loads(s), True\n",
    "    except Exception:\n",
    "        return {}, False\n",
    "\n",
    "def slot_f1_and_validity(pred_strs, gold_objs):\n",
    "    tp = fp = fn = 0\n",
    "    valid = 0\n",
    "    core = [\"ACTOR\", \"ACTION\", \"OBJECT\"]\n",
    "    all_slots = [\"ACTOR\",\"ACTION\",\"OBJECT\",\"LOCATION\",\"TIME\"]\n",
    "\n",
    "    for ps, g in zip(pred_strs, gold_objs):\n",
    "        pobj, ok = safe_json(ps)\n",
    "\n",
    "        # structural validity (keys exist) + non-empty core\n",
    "        if ok:\n",
    "            try:\n",
    "                Frame(**{k: pobj.get(k, \"\") for k in all_slots})\n",
    "                structural_ok = True\n",
    "            except ValidationError:\n",
    "                structural_ok = False\n",
    "        else:\n",
    "            structural_ok = False\n",
    "\n",
    "        nonempty_core = all(norm(pobj.get(k, \"\")) != \"\" for k in core)\n",
    "        if structural_ok and nonempty_core:\n",
    "            valid += 1\n",
    "\n",
    "        # micro Slot-F1 over the 5 slots (exact string match after normalization)\n",
    "        for k in all_slots:\n",
    "            p = norm(pobj.get(k, \"\"))\n",
    "            t = norm(g.get(k, \"\"))\n",
    "            if p == \"\" and t == \"\":\n",
    "                continue\n",
    "            if p != \"\" and t != \"\":\n",
    "                if p == t:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    fn += 1\n",
    "            elif p != \"\" and t == \"\":\n",
    "                fp += 1\n",
    "            elif p == \"\" and t != \"\":\n",
    "                fn += 1\n",
    "\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return f1, valid / max(1, len(gold_objs))\n",
    "\n",
    "# ----- prepare inputs -----\n",
    "texts = [ex[\"input_text\"] for ex in test_io]\n",
    "enc = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "\n",
    "device = next(model.parameters()).device if hasattr(model, \"parameters\") else torch.device(\"cpu\")\n",
    "enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "# ----- generate -----\n",
    "with torch.no_grad():\n",
    "    out_ids = model.generate(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc.get(\"attention_mask\", None),\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "pred_texts = tok.batch_decode(out_ids, skip_special_tokens=True)\n",
    "\n",
    "# ----- load gold -----\n",
    "gold_objs = []\n",
    "for x in test_io:\n",
    "    tgt = x.get(\"target_text\", \"\")\n",
    "    try:\n",
    "        gold_objs.append(json.loads(tgt))\n",
    "    except Exception:\n",
    "        # very lenient fallback (handles tiny formatting quirks)\n",
    "        obj, ok = safe_json(tgt)\n",
    "        gold_objs.append(obj if ok else {\"ACTOR\":\"\",\"ACTION\":\"\",\"OBJECT\":\"\",\"LOCATION\":\"\",\"TIME\":\"\"})\n",
    "\n",
    "# ----- metrics -----\n",
    "slot_f1, frame_valid = slot_f1_and_validity(pred_texts, gold_objs)\n",
    "print(f\"Slot-F1: {slot_f1:.3f}\")\n",
    "print(f\"Frame-Validity %: {frame_valid*100:.1f}%\")\n",
    "\n",
    "# ----- quick peek -----\n",
    "n_show = min(5, len(test_io))\n",
    "for i in range(n_show):\n",
    "    print(\"\\nText:\", test_io[i][\"input_text\"].replace(\"extract slots: \",\"\"))\n",
    "    print(\"Pred:\", pred_texts[i])\n",
    "    print(\"Gold:\", test_io[i][\"target_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb0764",
   "metadata": {},
   "source": [
    "## Result Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11f93d",
   "metadata": {},
   "source": [
    "**Slot-F1 = 0.374**\n",
    "\n",
    "This metric tells you how accurately each slot (ACTOR, ACTION, OBJECT, LOCATION, TIME) was predicted in the generated JSON output.\n",
    "\n",
    "A Slot-F1 of 0.374 means the model is identifying more slots correctly than before, but still makes notable mistakes such as swapping roles or producing near-miss words (e.g., Explics instead of Explains).\n",
    "\n",
    "This result is reasonable for your current dataset because:\n",
    "\n",
    "The dataset remains small (327 sentences total).\n",
    "\n",
    "Many sentences are short and lack clear ACTION or LOCATION cues.\n",
    "\n",
    "The model must learn to both understand the sentence and generate valid JSON strings â€” a harder task than token tagging.\n",
    "\n",
    "So, the T5 model is capturing some semantic structure but still generalizes inconsistently, especially on rarer slots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc430a8",
   "metadata": {},
   "source": [
    "**Frame-Validity = 57.6 %**\n",
    "\n",
    "This measures how often the model outputs a fully structured and valid frame (i.e., ACTOR, ACTION, and OBJECT are all non-empty).\n",
    "\n",
    "A score of 57.6 % means that in roughly 3 out of 5 sentences, T5 generated a usable JSON frame; the remaining cases had formatting issues or missing slots.\n",
    "\n",
    "This lower validity rate is expected because text-generation models like T5 sometimes produce incomplete or slightly malformed outputs without decoding constraints.\n",
    "\n",
    "It shows that while the model understands the relationships conceptually, it still struggles to enforce consistent structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c20a6",
   "metadata": {},
   "source": [
    "**Overall Summary**\n",
    "\n",
    "T5 improves Slot-F1 compared to BERT (0.374 vs 0.35), showing better per-slot accuracy.\n",
    "\n",
    "BERT still leads in Frame-Validity (81.8 % vs 57.6 %), meaning its outputs are structurally more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7065e",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The T5 model demonstrates promising flexibility â€” it can directly generate structured frames from text â€” but at the cost of output stability.\n",
    "Its higher Slot-F1 suggests it learned useful mappings between text and slot meanings, while the drop in Frame-Validity highlights that generation quality still needs refinement (e.g., stricter decoding or post-processing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c40615",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bfa494",
   "metadata": {},
   "source": [
    "## Comparison: BERT vs T5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7d1df",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "Both models were trained on the same dataset of 327 annotated sentences, but they use different strategies for extracting frames.\n",
    "\n",
    "**BERT** learns at the token level. It tags each word as ACTOR, ACTION, OBJECT, etc., and then reconstructs frames from those tags.\n",
    "\n",
    "- Strength: Produces more structurally valid outputs (81.8 %).\n",
    "\n",
    "- Limitation: Slightly lower Slot-F1 (0.35) due to local boundary errors and limited context.\n",
    "\n",
    "**T5** directly generates structured JSON outputs from raw sentences.\n",
    "\n",
    "- Strength: Achieved a higher Slot-F1 (0.374), showing better overall slot accuracy.\n",
    "\n",
    "- Limitation: Lower Frame-Validity (57.6 %), as some generated frames were incomplete or malformed.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "T5 demonstrates better slot-level understanding but is less consistent structurally.\n",
    "BERT, while slightly less accurate on slot values, maintains stronger control over frame format and validity.\n",
    "This shows a clear trade-off between flexibility (T5) and structural reliability (BERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9fb704",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-it3386] *",
   "language": "python",
   "name": "conda-env-.conda-it3386-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
