{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56c18b7",
   "metadata": {},
   "source": [
    "## FYP Sprint 3 ML training\n",
    "\n",
    "### Ian Chia \n",
    "### 230746D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a261232",
   "metadata": {},
   "source": [
    "### Mini GamePlan Idea to give the teacher some sort of understanding of what i am trying to do \n",
    "\n",
    "1) Build a mini version of the ML pipeline\n",
    "\n",
    "\n",
    "2) Define the shape of the JSON\n",
    "\n",
    "\n",
    "3) Use a tiny dataset (few examples made up on the spot) Why? : So we can test the full flow without risking the real MongoDB data\n",
    "\n",
    "\n",
    "4) Connect to your real data (350 annotated examples) : Once the testing is complete we will replace everything with the real one.\n",
    "\n",
    "### Why are we doing this:\n",
    "\n",
    "So right now, we just use the mini version because:\n",
    "\n",
    "It’s faster — we don’t need to connect to MongoDB yet.\n",
    "\n",
    "It’s safer — we can test code without touching your real data.\n",
    "\n",
    "It’s simple — we only need 5 core slots to prove the system works.\n",
    "\n",
    "Once it works, we’ll swap in the real schema (which already lives in your app)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc1858",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b22cbd",
   "metadata": {},
   "source": [
    "### Mini Testing pipeline :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c513247",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcf7ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Cell 0 — Install some libraries. \n",
    "\n",
    "#### Downloading the tools needed to make it work:\n",
    "transformers = lets you use models like BERT.\n",
    "\n",
    "datasets = helps handle small text datasets.\n",
    "\n",
    "seqeval = measures how well BERT tags words (F1 score).\n",
    "\n",
    "jsonschema = checks if your output JSON follows your rulebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a661b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets seqeval jsonschema accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05bf55",
   "metadata": {},
   "source": [
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3fd1a",
   "metadata": {},
   "source": [
    "### Cell 1 — Import the needed libraries & Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9f8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import json, re, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "#Cell 8 - training\n",
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee413a",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8c346",
   "metadata": {},
   "source": [
    "### Cell 2 — Define Schema & Mini Examples\n",
    "\n",
    "define slots  : These are the slots that we want to use\n",
    "\n",
    "bio labels    : BIO labels are just a way to teach the model which words belong to which slot.\n",
    "\n",
    "define schema : CASE_FRAME_SCHEMA = {\n",
    "\n",
    "mini examples : examples = [\n",
    "\n",
    "split data    : train / validation / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c4915b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slot labels we’ll predict\n",
    "SLOTS = [\"ACTOR\",\"ACTION\",\"OBJECT\",\"LOCATION\",\"TIME\"]\n",
    "BIO_LABELS = [\"O\"] + [f\"{p}-{s}\" for s in SLOTS for p in [\"B\",\"I\"]]\n",
    "LABEL2ID = {lab:i for i,lab in enumerate(BIO_LABELS)}\n",
    "ID2LABEL = {i:lab for lab,i in LABEL2ID.items()}\n",
    "\n",
    "# minimal schema (acceptance: ACTOR & ACTION required for \"valid frame\")\n",
    "CASE_FRAME_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {s: {\"type\": [\"string\",\"null\"]} for s in SLOTS},\n",
    "    \"required\": [\"ACTOR\",\"ACTION\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "# Tiny seed set to start (replace with your own later)\n",
    "examples = [\n",
    "    (\"Alice kicked the ball at the park yesterday.\",\n",
    "     {\"ACTOR\":\"Alice\",\"ACTION\":\"kicked\",\"OBJECT\":\"the ball\",\"LOCATION\":\"the park\",\"TIME\":\"yesterday\"}),\n",
    "    (\"Bob repaired the bike in the garage last night.\",\n",
    "     {\"ACTOR\":\"Bob\",\"ACTION\":\"repaired\",\"OBJECT\":\"the bike\",\"LOCATION\":\"the garage\",\"TIME\":\"last night\"}),\n",
    "    (\"Chloe reads a novel at home every morning.\",\n",
    "     {\"ACTOR\":\"Chloe\",\"ACTION\":\"reads\",\"OBJECT\":\"a novel\",\"LOCATION\":\"home\",\"TIME\":\"every morning\"}),\n",
    "    (\"Daniel cooked pasta in the kitchen at noon.\",\n",
    "     {\"ACTOR\":\"Daniel\",\"ACTION\":\"cooked\",\"OBJECT\":\"pasta\",\"LOCATION\":\"the kitchen\",\"TIME\":\"at noon\"}),\n",
    "    (\"Eva painted the fence outside on Sunday.\",\n",
    "     {\"ACTOR\":\"Eva\",\"ACTION\":\"painted\",\"OBJECT\":\"the fence\",\"LOCATION\":\"outside\",\"TIME\":\"on Sunday\"}),\n",
    "]\n",
    "\n",
    "# simple split: 3 train, 1 dev, 1 test (tiny on purpose)\n",
    "random.seed(3)\n",
    "random.shuffle(examples)\n",
    "train_ex = examples[:3]\n",
    "dev_ex   = examples[3:4]\n",
    "test_ex  = examples[4:5]\n",
    "\n",
    "len(train_ex), len(dev_ex), len(test_ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f92b88",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb168c4a",
   "metadata": {},
   "source": [
    "### Cell 3 — Helper: build simple BIO tags from the gold JSON (whitespace matching)\n",
    "\n",
    "It takes each sentence and its correct answer (the JSON), and colors every word with a tag:\n",
    "\n",
    "B-ACTOR = first word of the ACTOR\n",
    "\n",
    "I-ACTOR = the rest of the ACTOR words\n",
    "\n",
    "… same for ACTION / OBJECT / LOCATION / TIME\n",
    "\n",
    "O = not part of any slot\n",
    "\n",
    "So later, BERT can learn: “when I see words like this, color them like that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83853b5c",
   "metadata": {},
   "source": [
    "### More In-dept explanation:\n",
    "\n",
    "\n",
    "**bio_tag(text, target_json)**\n",
    "\n",
    "**Input:**\n",
    "\n",
    "text = the sentence (e.g., “Alice kicked the ball…”)\n",
    "\n",
    "target_json = the correct slots (e.g., ACTOR=Alice, ACTION=kicked,…)\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "Splits the sentence into tokens (words)\n",
    "\n",
    "Tries to find each slot phrase in the sentence\n",
    "(e.g., OBJECT = “the ball” → it looks for the + ball)\n",
    "\n",
    "Gives each word a label like B-OBJECT or I-OBJECT\n",
    "\n",
    "**Output:**\n",
    "\n",
    "tokens = list of words\n",
    "\n",
    "labels = list of tags (same length as tokens)\n",
    "\n",
    "Think “token list” = your word list, “label list” = your color list.\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "**to_records(pairs)**\n",
    "\n",
    "**Input:** a list of (text, gold_json) pairs\n",
    "\n",
    "**What it does:** calls bio_tag for each pair and packs everything into a neat record:\n",
    "\n",
    "{\"tokens\":[...], \"labels\":[...], \"text\": \"...\", \"target_json\": {...}}\n",
    "\n",
    "**Output:** a list of these neat records (for train/dev/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef8be4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Alice',\n",
       "  'kicked',\n",
       "  'the',\n",
       "  'ball',\n",
       "  'at',\n",
       "  'the',\n",
       "  'park',\n",
       "  'yesterday.'],\n",
       " 'labels': ['B-ACTOR',\n",
       "  'B-ACTION',\n",
       "  'B-OBJECT',\n",
       "  'I-OBJECT',\n",
       "  'O',\n",
       "  'B-LOCATION',\n",
       "  'I-LOCATION',\n",
       "  'B-TIME'],\n",
       " 'text': 'Alice kicked the ball at the park yesterday.',\n",
       " 'target_json': {'ACTOR': 'Alice',\n",
       "  'ACTION': 'kicked',\n",
       "  'OBJECT': 'the ball',\n",
       "  'LOCATION': 'the park',\n",
       "  'TIME': 'yesterday'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bio_tag(text: str, target_json: Dict[str,str]):\n",
    "    toks = text.split()\n",
    "    labels = [\"O\"]*len(toks)\n",
    "\n",
    "    def norm(x): return re.sub(r\"[^\\w']+\", \"\", x.lower())\n",
    "\n",
    "    for slot, phrase in target_json.items():\n",
    "        if not phrase:\n",
    "            continue\n",
    "        p_tokens = phrase.split()\n",
    "        n = len(p_tokens)\n",
    "        # find first exact (punctuation-stripped) match\n",
    "        for i in range(len(toks)-n+1):\n",
    "            window = [norm(t) for t in toks[i:i+n]]\n",
    "            if window == [norm(t) for t in p_tokens]:\n",
    "                labels[i] = f\"B-{slot}\"\n",
    "                for j in range(1, n):\n",
    "                    labels[i+j] = f\"I-{slot}\"\n",
    "                break\n",
    "    return toks, labels\n",
    "\n",
    "def to_records(pairs):\n",
    "    recs = []\n",
    "    for text, gold in pairs:\n",
    "        tokens, labels = bio_tag(text, gold)\n",
    "        recs.append({\"tokens\": tokens, \"labels\": labels, \"text\": text, \"target_json\": gold})\n",
    "    return recs\n",
    "\n",
    "train_recs = to_records(train_ex)\n",
    "dev_recs   = to_records(dev_ex)\n",
    "test_recs  = to_records(test_ex)\n",
    "\n",
    "train_recs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007ccdf",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcc51e",
   "metadata": {},
   "source": [
    "### Cell 4 —  Build Hugging Face Datasets\n",
    "\n",
    "group bio_tag records into three boxes:\n",
    "\n",
    "a train box (to learn),\n",
    "\n",
    "a validation/dev box (to check while learning),\n",
    "\n",
    "a test box (final quiz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6689345a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'labels', 'text', 'target_json'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'labels', 'text', 'target_json'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'labels', 'text', 'target_json'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_recs),\n",
    "    \"validation\": Dataset.from_list(dev_recs),\n",
    "    \"test\": Dataset.from_list(test_recs)\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b29f17",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f4662",
   "metadata": {},
   "source": [
    "### Cell 5 — Tokenizer and label alignment (wordpieces → BIO ids)\n",
    " \n",
    "It turns word tokens into BERT's subword pieces and carry the BIO labels over correctly\n",
    "\n",
    "**tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)**\n",
    "\n",
    "Loads BERT’s tokenizer (splits words into subword pieces like “play” + “##ing”).\n",
    "\n",
    "**tokenized = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)**\n",
    "\n",
    "Tokenizes lists of words and keeps a mapping from subword → original word.\n",
    "\n",
    "**word_ids = tokenized.word_ids(batch_index=i)**\n",
    "\n",
    "Lets us know which subwords belong to which original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375620e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 42.00 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 78.70 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 71.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 1, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def align_labels_with_tokens(examples):\n",
    "    tokenized = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        prev_word = None\n",
    "        label_ids = []\n",
    "        for w_id in word_ids:\n",
    "            if w_id is None:\n",
    "                label_ids.append(-100)  # ignore special tokens\n",
    "            else:\n",
    "                lab = labels[w_id]\n",
    "                # if it's a continuation of a wordpiece, keep I- if B- was there\n",
    "                if w_id == prev_word:\n",
    "                    if lab.startswith(\"B-\"):\n",
    "                        lab = \"I-\" + lab[2:]\n",
    "                label_ids.append(LABEL2ID[lab])\n",
    "                prev_word = w_id\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = new_labels\n",
    "    return tokenized\n",
    "\n",
    "#Applies this alignment to train/dev/test.\n",
    "tokenized_ds = ds.map(align_labels_with_tokens, batched=True)\n",
    "len(tokenized_ds[\"train\"]), len(tokenized_ds[\"validation\"]), len(tokenized_ds[\"test\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3bd43",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b238e",
   "metadata": {},
   "source": [
    "### Cell 6 — load BERT for token classification\n",
    "\n",
    "Create a BERT model head that predcits a BIO label for each subword\n",
    "\n",
    "Loads BERT + a classification layer with exactly the number of BIO labels you defined.\n",
    "\n",
    "**data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)**\n",
    "Handles dynamic padding so batches are the same length (important for training).\n",
    "\n",
    "ETUDS: in cell 6 the model will try to learn to predict one label per token, for example wehn it looks at a person name it will try to learn that its a person name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2600c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ian Chia\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(BIO_LABELS),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bef2fe",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ca318",
   "metadata": {},
   "source": [
    "### Cell 7 — metrics for token labels (seqeval)\n",
    "\n",
    "Goal: after the model predicts, compute sequence tagging metrics correctly.We want to measure how close the guesses are to the real answers. \n",
    "\n",
    "We compare those guesses with the true BIO labels you made in Cell 3.\n",
    "\n",
    "Using the seqeval library, we calculate:\n",
    "\n",
    "- Precision = how many of the model’s “colored” words were correct.\n",
    "\n",
    "- Recall = how many of the real colored words it found.\n",
    "\n",
    "- F1 = balanced score between the two.\n",
    "\n",
    "**preds = np.argmax(logits, axis=-1)**\n",
    "Turns model scores into predicted label IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef37c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_tags, pred_tags = [], []\n",
    "    for p, l in zip(preds, labels):\n",
    "        cur_true, cur_pred = [], []\n",
    "        for pi, li in zip(p, l):\n",
    "            if li == -100:\n",
    "                continue\n",
    "            cur_true.append(ID2LABEL[li])\n",
    "            cur_pred.append(ID2LABEL[pi])\n",
    "        true_tags.append(cur_true)\n",
    "        pred_tags.append(cur_pred)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_tags, pred_tags),\n",
    "        \"recall\": recall_score(true_tags, pred_tags),\n",
    "        \"f1\": f1_score(true_tags, pred_tags),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ab72b",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eece2e",
   "metadata": {},
   "source": [
    "### Cell 8 — train (tiny, fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "165db216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\AppData\\Local\\Temp\\ipykernel_19644\\3988588579.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1785221099853516,\n",
       " 'eval_precision': 0.14285714285714285,\n",
       " 'eval_recall': 0.2,\n",
       " 'eval_f1': 0.16666666666666666,\n",
       " 'eval_runtime': 0.2741,\n",
       " 'eval_samples_per_second': 3.648,\n",
       " 'eval_steps_per_second': 3.648,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./models/bert_slot_tagger\",\n",
    "    logging_dir=\"./models/bert_slot_tagger/logs\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_token_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd84505",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "What happened: BERT learned from the tiny “study notes” (3 sentences) and was quizzed on 1 validation sentence.\n",
    "\n",
    "**Key fields that can be seen:**\n",
    "\n",
    "eval_loss: how wrong the model still is (lower is better).\n",
    "\n",
    "eval_precision, eval_recall, eval_f1: how well it colored the words with the right BIO tags (higher is better).\n",
    "\n",
    "**Why are they low?** I only trained on 3 sentences on CPU. That’s not enough for BERT to learn patterns; it’s just to **prove the pipeline runs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d88539",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26528dfd",
   "metadata": {},
   "source": [
    "### Cell 9 — predict BIO tags on the test set + assemble JSON\n",
    "What it does: uses the trained model to “color” the test sentence(s), then turns colors → case-frame JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d254224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bob repaired the bike in the garage last night.',\n",
       "  {'ACTOR': None,\n",
       "   'ACTION': None,\n",
       "   'OBJECT': 'bike',\n",
       "   'LOCATION': 'repaired',\n",
       "   'TIME': 'the'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) get model predictions on tokenized test data\n",
    "tok_test = tokenized_ds[\"test\"]\n",
    "raw_test = ds[\"test\"]  # has original tokens/text/gold\n",
    "\n",
    "pred_logits = trainer.predict(tok_test).predictions\n",
    "pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "# 2) map predicted ids back to BIO labels (skip -100 positions)\n",
    "pred_bio = []\n",
    "for row_pred, row_labels in zip(pred_ids, tok_test[\"labels\"]):\n",
    "    labs = []\n",
    "    k = 0\n",
    "    for li in row_labels:\n",
    "        if li == -100:\n",
    "            continue\n",
    "        labs.append(ID2LABEL[row_pred[k]])\n",
    "        k += 1\n",
    "    pred_bio.append(labs)\n",
    "\n",
    "# 3) assemble JSON from BIO tags\n",
    "def assemble_json_from_bio(tokens, labels):\n",
    "    out = {s: None for s in SLOTS}\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        lab = labels[i]\n",
    "        if lab.startswith(\"B-\"):\n",
    "            slot = lab.split(\"-\")[1]\n",
    "            # collect I- continuation\n",
    "            j = i + 1\n",
    "            span = [tokens[i]]\n",
    "            while j < len(tokens) and labels[j] == f\"I-{slot}\":\n",
    "                span.append(tokens[j]); j += 1\n",
    "            phrase = \" \".join(span).strip(\" ,.\")\n",
    "            if out[slot] is None:  # take first span per slot\n",
    "                out[slot] = phrase\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "pred_jsons = []\n",
    "for idx in range(len(raw_test)):\n",
    "    tokens = raw_test[idx][\"tokens\"]\n",
    "    labels = pred_bio[idx]\n",
    "    pred_jsons.append(assemble_json_from_bio(tokens, labels))\n",
    "\n",
    "list(zip([r[\"text\"] for r in raw_test], pred_jsons))  # quick peek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b6b02",
   "metadata": {},
   "source": [
    "### Explanation: \n",
    "\n",
    "**What happened:** The model guessed BIO tags, we converted them to a JSON.\n",
    "\n",
    "Why does it look wrong? With almost no training data, the model “colored” badly:\n",
    "\n",
    "- It missed ACTOR and ACTION\n",
    "\n",
    "- Misplaced LOCATION (“repaired”) and TIME (“the”)\n",
    "\n",
    "- Only OBJECT (“bike”) came out somewhat okay\n",
    "\n",
    "This is expected with such a tiny toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f21a15",
   "metadata": {},
   "source": [
    "---------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03481473",
   "metadata": {},
   "source": [
    "### Cell 10 — validate with schema + compute simple Slot-F1 + Frame-Validity\n",
    "What it does: checks if the JSON is well-formed and measures how close each slot is to the gold answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca977021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'slot_f1_avg': 0.0,\n",
       " 'frame_validity_pct': 100.0,\n",
       " 'per_item': [{'text': 'Bob repaired the bike in the garage last night.',\n",
       "   'pred': {'ACTOR': None,\n",
       "    'ACTION': None,\n",
       "    'OBJECT': 'bike',\n",
       "    'LOCATION': 'repaired',\n",
       "    'TIME': 'the'},\n",
       "   'gold': {'ACTION': 'repaired',\n",
       "    'ACTOR': 'Bob',\n",
       "    'LOCATION': 'the garage',\n",
       "    'OBJECT': 'the bike',\n",
       "    'TIME': 'last night'},\n",
       "   'is_schema_valid': True,\n",
       "   'slot_f1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "def is_schema_valid(cf):\n",
    "    try:\n",
    "        validate(cf, CASE_FRAME_SCHEMA)\n",
    "        return True\n",
    "    except ValidationError:\n",
    "        return False\n",
    "\n",
    "def slot_f1(pred, gold):\n",
    "    # exact string match per slot (case-insensitive)\n",
    "    tp = fp = fn = 0\n",
    "    for s in SLOTS:\n",
    "        p = (pred.get(s) or \"\").strip().lower()\n",
    "        g = (gold.get(s) or \"\").strip().lower()\n",
    "        if p and g and p == g:\n",
    "            tp += 1\n",
    "        elif p and g and p != g:\n",
    "            fp += 1; fn += 1\n",
    "        elif p and not g:\n",
    "            fp += 1\n",
    "        elif not p and g:\n",
    "            fn += 1\n",
    "    prec = tp / (tp+fp) if (tp+fp) else 0.0\n",
    "    rec  = tp / (tp+fn) if (tp+fn) else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "    return {\"precision\":prec, \"recall\":rec, \"f1\":f1}\n",
    "\n",
    "texts = [r[\"text\"] for r in raw_test]\n",
    "golds = [r[\"target_json\"] for r in raw_test]\n",
    "valid_flags = [is_schema_valid(pj) for pj in pred_jsons]\n",
    "slot_scores = [slot_f1(pj, gj) for pj, gj in zip(pred_jsons, golds)]\n",
    "\n",
    "results = {\n",
    "    \"slot_f1_avg\": float(np.mean([s[\"f1\"] for s in slot_scores])),\n",
    "    \"frame_validity_pct\": 100.0 * sum(valid_flags) / len(valid_flags),\n",
    "    \"per_item\": [\n",
    "        {\"text\": t, \"pred\": p, \"gold\": g, \"is_schema_valid\": v, \"slot_f1\": s}\n",
    "        for t,p,g,v,s in zip(texts, pred_jsons, golds, valid_flags, slot_scores)\n",
    "    ]\n",
    "}\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89488a4b",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "**slot_f1_avg: 0.0** → Across the five slots, none matched the gold text exactly on this one test example, so average F1 is 0.\n",
    "\n",
    "**frame_validity_pct: 100.0** → The JSON format is valid (keys exist and types are allowed).\n",
    "Note: our schema allows null, so the frame can be “valid shape” even if content is missing. That’s by design, so the pipeline never crashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63f283",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1439c",
   "metadata": {},
   "source": [
    "### Cell 11 — save artifacts (metrics + predictions)\n",
    "\n",
    "What it does: writes outputs to files so you can paste screenshots/tables into your proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b39a485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved to results/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "with open(\"results/metrics.json\",\"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "with open(\"results/predictions.jsonl\",\"w\") as f:\n",
    "    for t, p, g, v in zip(texts, pred_jsons, golds, valid_flags):\n",
    "        f.write(json.dumps({\"text\":t, \"pred_json\":p, \"gold_json\":g, \"is_schema_valid\":v})+\"\\n\")\n",
    "\"saved to results/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253179ef",
   "metadata": {},
   "source": [
    "This saved two files:\n",
    "\n",
    "results/metrics.json (your scores)\n",
    "\n",
    "results/predictions.jsonl (text, predicted JSON, gold JSON, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c82bd81",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303d121",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "We successfully proved the full BERT pipeline: text → BIO tags → assembled JSON → schema validation → metrics. On a tiny toy set (3 train / 1 dev / 1 test), scores are low (e.g., slot_f1_avg ≈ 0) and predictions are noisy, which is expected because the model had almost no data to learn from. The output JSONs are still schema-valid, confirming the pipeline is robust and won’t crash.\n",
    "\n",
    "This is not the final model. Next, we will (1) try the T5 track (direct text→JSON) for comparison, and (2) train again using our real 350 annotated examples and the real schema file. With more data and a few training tweaks (more epochs, appropriate learning rate), we expect Slot-F1 and Frame-Validity to improve significantly. The best-performing track (BERT or T5) will be carried into the proposal with metrics and example outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf70894",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-it3386] *",
   "language": "python",
   "name": "conda-env-.conda-it3386-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
