{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e621fd61",
   "metadata": {},
   "source": [
    "## FYP Sprint 4 \n",
    "### Ian Chia \n",
    "### 230746D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a431ee6",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b402b3",
   "metadata": {},
   "source": [
    "### T0.1 : Creating a fixed split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b3cb2",
   "metadata": {},
   "source": [
    "### Cell 1 â€” Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78afe0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e3703c",
   "metadata": {},
   "source": [
    "### Cell 2 â€” Load dataset_full.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ec197b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A selfish, whiny teenaged girl</td>\n",
       "      <td>{'ACTOR': 'girl', 'ACTION': '', 'OBJECT': '', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When a character delivers a speech so powerful...</td>\n",
       "      <td>{'ACTOR': 'character', 'ACTION': 'delivers', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An ugly (in personality or appearance)/overwei...</td>\n",
       "      <td>{'ACTOR': 'personality', 'ACTION': 'knows', 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sÃ¡Â»Â©c khoÃ¡ÂºÂ» tÃ¡Â»â€¢ng Ã„â€˜ÃƒÂ n Ã¡Â»â€¢n Ã„â€˜Ã¡Â»â€¹nh, tÃ¡Â»â€° l...</td>\n",
       "      <td>{'ACTOR': 'Ã„â€˜Ãƒ', 'ACTION': 'hÃ¡ÂºÂ¥p', 'OBJECT': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A movie adaptation of a novel</td>\n",
       "      <td>{'ACTOR': 'novel', 'ACTION': '', 'OBJECT': 'no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                     A selfish, whiny teenaged girl   \n",
       "1  When a character delivers a speech so powerful...   \n",
       "2  An ugly (in personality or appearance)/overwei...   \n",
       "3  sÃ¡Â»Â©c khoÃ¡ÂºÂ» tÃ¡Â»â€¢ng Ã„â€˜ÃƒÂ n Ã¡Â»â€¢n Ã„â€˜Ã¡Â»â€¹nh, tÃ¡Â»â€° l...   \n",
       "4                      A movie adaptation of a novel   \n",
       "\n",
       "                                         target_json  \n",
       "0  {'ACTOR': 'girl', 'ACTION': '', 'OBJECT': '', ...  \n",
       "1  {'ACTOR': 'character', 'ACTION': 'delivers', '...  \n",
       "2  {'ACTOR': 'personality', 'ACTION': 'knows', 'O...  \n",
       "3  {'ACTOR': 'Ã„â€˜Ãƒ', 'ACTION': 'hÃ¡ÂºÂ¥p', 'OBJECT': ...  \n",
       "4  {'ACTOR': 'novel', 'ACTION': '', 'OBJECT': 'no...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"dataset_full.jsonl\", lines=True)\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b7c86",
   "metadata": {},
   "source": [
    "### Cell 3 â€” Shuffle once with a fixed seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68c69d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_shuffled.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ac5a5",
   "metadata": {},
   "source": [
    "### Cell 4 â€” Create split sizes (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78673fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327, 228, 49, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_total = len(df_shuffled)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_dev = int(0.15 * n_total)\n",
    "n_test = n_total - n_train - n_dev\n",
    "\n",
    "n_total, n_train, n_dev, n_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac3604",
   "metadata": {},
   "source": [
    "### Cell 5 â€” Assign the split column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01bca160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    228\n",
       "test      50\n",
       "dev       49\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled[\"split\"] = \"train\"\n",
    "\n",
    "df_shuffled.loc[n_train:n_train+n_dev-1, \"split\"] = \"dev\"\n",
    "df_shuffled.loc[n_train+n_dev:, \"split\"] = \"test\"\n",
    "\n",
    "df_shuffled[\"split\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee040d65",
   "metadata": {},
   "source": [
    "### Cell 6 â€” Save the official Sprint 4 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dcd7350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled.to_json(\n",
    "    \"idea_annotator_sprint4_split_fixed.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "###This file becomes your master dataset for the entire Sprint 4 pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776e4f9",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b557c31",
   "metadata": {},
   "source": [
    "### T0.2A: BERT Baseline on Fixed Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95811511",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c745233",
   "metadata": {},
   "source": [
    "Sprint 3 used old preprocessed training files that already had BIO tags.\n",
    "\n",
    "Sprint 4 uses the real, raw MongoDB dataset, which only contains text + JSON â€”\n",
    "so we had to regenerate BIO tags before training BERT.\n",
    "\n",
    "**Sprint 3 dataset**\n",
    "\n",
    "I exported from MongoDB BUT\n",
    "\n",
    "I used a processed version\n",
    "\n",
    "This version contained \"tokens\" + \"BIO tags\"\n",
    "\n",
    "This means BIO tags had already been generated earlier,  during Sprint 3\n",
    "\n",
    "So Sprint 3 BERT ran easily\n",
    "\n",
    "**Sprint 4 dataset**\n",
    "\n",
    "The file i am  using (dataset_full.jsonl)\n",
    "is a different export that contains the raw annotation (target_json)\n",
    "\n",
    "This version does not contain BIO tags\n",
    "\n",
    "So I  had to regenerate BIO tags from the JSON\n",
    "\n",
    "This also created a cleaner baseline (higher F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4eb07",
   "metadata": {},
   "source": [
    "### Cell 1 â€” Load the fixed split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d31ce2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 49, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"idea_annotator_sprint4_split_fixed.jsonl\", lines=True)\n",
    "train_df = df[df[\"split\"] == \"train\"]\n",
    "dev_df = df[df[\"split\"] == \"dev\"]\n",
    "test_df = df[df[\"split\"] == \"test\"]\n",
    "\n",
    "len(train_df), len(dev_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297abec0",
   "metadata": {},
   "source": [
    "### Cell 2 â€” Check what columns we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0560f0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['text', 'target_json', 'split'], dtype='object'),\n",
       "                                                 text  \\\n",
       " 0  An organization is improbably selective about ...   \n",
       " 1  Now, stop being lazy and go read the full arti...   \n",
       " 2        Word(s) appearing through an Arc as a Motif   \n",
       " \n",
       "                                          target_json  split  \n",
       " 0  {'ACTOR': 'organization', 'ACTION': 'is', 'OBJ...  train  \n",
       " 1  {'ACTOR': 'full article', 'ACTION': 'stop', 'O...  train  \n",
       " 2  {'ACTOR': 'Word(s )', 'ACTION': 'appearing', '...  train  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns, train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b4ab3",
   "metadata": {},
   "source": [
    "### Cell 3 â€” Convert raw text â†’ tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13402cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into simple whitespace tokens\n",
    "train_tokens = train_df[\"text\"].apply(lambda x: x.split()).tolist()\n",
    "dev_tokens   = dev_df[\"text\"].apply(lambda x: x.split()).tolist()\n",
    "test_tokens  = test_df[\"text\"].apply(lambda x: x.split()).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287dc07",
   "metadata": {},
   "source": [
    "### Cell 4 â€” Convert target_json â†’ BIO tags\n",
    "\n",
    "This cell:\n",
    "\n",
    "Loops through each sentenceâ€™s tokens\n",
    "\n",
    "Uses the gold JSON target\n",
    "\n",
    "Marks BIO tags for each slot\n",
    "\n",
    "Produces a list of labels per token (same format as Sprint 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad3af6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 49, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def json_to_bio(tokens, json_obj):\n",
    "    # Create a default tag list\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    # For each slot in the JSON (ACTOR, ACTION, etc.)\n",
    "    for slot, value in json_obj.items():\n",
    "        if value is None or value == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Value may contain multiple words, so split\n",
    "        value_tokens = value.split()\n",
    "\n",
    "        # Find matching positions in the token list\n",
    "        for i in range(len(tokens)):\n",
    "            # Check if tokens[i:i+len(value_tokens)] matches the slot value\n",
    "            if tokens[i:i+len(value_tokens)] == value_tokens:\n",
    "                tags[i] = f\"B-{slot}\"\n",
    "                for j in range(1, len(value_tokens)):\n",
    "                    tags[i+j] = f\"I-{slot}\"\n",
    "    \n",
    "    return tags\n",
    "\n",
    "# Apply function to build BIO labels\n",
    "train_labels = [\n",
    "    json_to_bio(toks, json_obj) \n",
    "    for toks, json_obj in zip(train_tokens, train_df[\"target_json\"])\n",
    "]\n",
    "\n",
    "dev_labels = [\n",
    "    json_to_bio(toks, json_obj) \n",
    "    for toks, json_obj in zip(dev_tokens, dev_df[\"target_json\"])\n",
    "]\n",
    "\n",
    "test_labels = [\n",
    "    json_to_bio(toks, json_obj) \n",
    "    for toks, json_obj in zip(test_tokens, test_df[\"target_json\"])\n",
    "]\n",
    "\n",
    "len(train_labels), len(dev_labels), len(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0ea64",
   "metadata": {},
   "source": [
    "### Cell 5 â€” Build label list and mappings\n",
    "\n",
    "This cell collects all unique BIO tags from the train set, and builds label2id / id2label dictionaries for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0d87d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['B-ACTION',\n",
       "  'B-ACTOR',\n",
       "  'B-LOCATION',\n",
       "  'B-OBJECT',\n",
       "  'I-ACTION',\n",
       "  'I-ACTOR',\n",
       "  'I-LOCATION',\n",
       "  'I-OBJECT',\n",
       "  'O'],\n",
       " {'B-ACTION': 0,\n",
       "  'B-ACTOR': 1,\n",
       "  'B-LOCATION': 2,\n",
       "  'B-OBJECT': 3,\n",
       "  'I-ACTION': 4,\n",
       "  'I-ACTOR': 5,\n",
       "  'I-LOCATION': 6,\n",
       "  'I-OBJECT': 7,\n",
       "  'O': 8})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect labels from ALL splits\n",
    "all_labels = set()\n",
    "\n",
    "for label_seq in train_labels + dev_labels + test_labels:\n",
    "    for tag in label_seq:\n",
    "        all_labels.add(tag)\n",
    "\n",
    "# Sort for stable order\n",
    "label_list = sorted(list(all_labels))\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "label_list, label2id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650342f5",
   "metadata": {},
   "source": [
    "### Cell 6 â€” Tokenize with BERT tokenizer\n",
    "\n",
    "We now convert your token lists (train_tokens, dev_tokens, test_tokens) into BERT input IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465e85ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ian Chia\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Encode tokens into BERT input format\n",
    "train_encodings = tokenizer(\n",
    "    train_tokens,\n",
    "    is_split_into_words=True,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "dev_encodings = tokenizer(\n",
    "    dev_tokens,\n",
    "    is_split_into_words=True,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_tokens,\n",
    "    is_split_into_words=True,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef430b3",
   "metadata": {},
   "source": [
    "### Cell 7 â€” Encode BIO labels to align with WordPiece tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468fe042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 49, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_tags(labels_list, encodings):\n",
    "    encoded_labels = []\n",
    "    \n",
    "    for i, sentence_labels in enumerate(labels_list):\n",
    "        word_ids = encodings.word_ids(batch_index=i)\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens get ignored by loss\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First subword of a given token\n",
    "                label_ids.append(label2id[sentence_labels[word_idx]])\n",
    "            else:\n",
    "                # Subsequent subword piece (same label)\n",
    "                label_ids.append(label2id[sentence_labels[word_idx]])\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        encoded_labels.append(label_ids)\n",
    "    \n",
    "    return encoded_labels\n",
    "\n",
    "# Encode all label sets\n",
    "train_labels_enc = encode_tags(train_labels, train_encodings)\n",
    "dev_labels_enc   = encode_tags(dev_labels, dev_encodings)\n",
    "test_labels_enc  = encode_tags(test_labels, test_encodings)\n",
    "\n",
    "len(train_labels_enc), len(dev_labels_enc), len(test_labels_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c88122",
   "metadata": {},
   "source": [
    "### Cell 8 â€” Build the PyTorch Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd5f3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 49, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class IdeaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = IdeaDataset(train_encodings, train_labels_enc)\n",
    "dev_dataset   = IdeaDataset(dev_encodings, dev_labels_enc)\n",
    "test_dataset  = IdeaDataset(test_encodings, test_labels_enc)\n",
    "\n",
    "len(train_dataset), len(dev_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe2cb5",
   "metadata": {},
   "source": [
    "### Cell 9 â€” Load BERT model + define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d6fcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load BERT base model with correct number of labels\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Training settings (same as Sprint 3)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_s4_outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32031b8",
   "metadata": {},
   "source": [
    "### Cell 10 â€” Define compute_metrics (F1 + accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a13bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        curr_true = []\n",
    "        curr_pred = []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l == -100:  # ignore special tokens\n",
    "                continue\n",
    "            curr_true.append(id2label[l])\n",
    "            curr_pred.append(id2label[p])\n",
    "        true_labels.append(curr_true)\n",
    "        true_preds.append(curr_pred)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8611c6",
   "metadata": {},
   "source": [
    "### Cell 11 â€” Create the Trainer and start training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45914cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145/145 14:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.876613</td>\n",
       "      <td>0.697297</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.720884</td>\n",
       "      <td>0.745946</td>\n",
       "      <td>0.404580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.726624</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.480916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.753625</td>\n",
       "      <td>0.763964</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.785769</td>\n",
       "      <td>0.760360</td>\n",
       "      <td>0.462094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=145, training_loss=0.567440677511281, metrics={'train_runtime': 879.1885, 'train_samples_per_second': 1.297, 'train_steps_per_second': 0.165, 'total_flos': 38982635435880.0, 'train_loss': 0.567440677511281, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,      # evaluate on dev each epoch\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5085d",
   "metadata": {},
   "source": [
    "### Cell 12 â€” Evaluate on dev + test (get baseline metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ee54102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV BASELINE: {'eval_loss': 0.7266241312026978, 'eval_accuracy': 0.7747747747747747, 'eval_f1': 0.48091603053435117, 'eval_runtime': 1.8782, 'eval_samples_per_second': 26.089, 'eval_steps_per_second': 3.727, 'epoch': 5.0}\n",
      "TEST BASELINE: {'eval_loss': 0.691470205783844, 'eval_accuracy': 0.7797147385103012, 'eval_f1': 0.460431654676259, 'eval_runtime': 4.1597, 'eval_samples_per_second': 12.02, 'eval_steps_per_second': 1.683, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on dev set\n",
    "dev_results = trainer.evaluate(eval_dataset=dev_dataset)\n",
    "print(\"DEV BASELINE:\", dev_results)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"TEST BASELINE:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc7d549b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot-F1: 0.507\n",
      "Frame-Validity %: 92.0%\n",
      "\n",
      "Text: When a character delivers a speech so powerful that it emotionally moves the others to take action and not lose hope\n",
      "Pred: { \"ACTOR\":\"character\", \"ACTION\":\"delivers\", \"OBJECT\":\"speech\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"character\", \"ACTION\":\"delivers\", \"OBJECT\":\"speech\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "\n",
      "Text: A character who very rarely or never shows any emotion\n",
      "Pred: { \"ACTOR\":\"character\", \"ACTION\":\"shows\", \"OBJECT\":\"emotion\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"character\", \"ACTION\":\"shows\", \"OBJECT\":\"emotion\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "\n",
      "Text: Limited color palette on purpose\n",
      "Pred: { \"ACTOR\":\"\", \"ACTION\":\"\", \"OBJECT\":\"Limited color palette\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"purpose\", \"ACTION\":\"\", \"OBJECT\":\"color palette\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "\n",
      "Text: A character comes home and finds that they cannot fit in there anymore\n",
      "Pred: { \"ACTOR\":\"character\", \"ACTION\":\"\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"character\", \"ACTION\":\"comes\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "\n",
      "Text: The best (or only) way to get rid of something is burning it\n",
      "Pred: { \"ACTOR\":\"\", \"ACTION\":\"burning\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"way\", \"ACTION\":\"burning\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will focus on these slots\n",
    "SLOT_NAMES = [\"ACTOR\", \"ACTION\", \"OBJECT\", \"LOCATION\", \"TIME\"]\n",
    "\n",
    "def bio_to_slots(tokens, tags):\n",
    "    \"\"\"\n",
    "    Convert BIO tag sequence into a simple slot dictionary:\n",
    "    { 'ACTOR': '...', 'ACTION': '...', ... }\n",
    "    \"\"\"\n",
    "    slots = {s: \"\" for s in SLOT_NAMES}\n",
    "    current_slot = None\n",
    "    current_tokens = []\n",
    "\n",
    "    def close_current():\n",
    "        nonlocal current_slot, current_tokens\n",
    "        if current_slot is not None and current_tokens and slots[current_slot] == \"\":\n",
    "            slots[current_slot] = \" \".join(current_tokens)\n",
    "        current_slot = None\n",
    "        current_tokens = []\n",
    "\n",
    "    for tok, tag in zip(tokens, tags):\n",
    "        if tag == \"O\" or tag == \"PAD\" or tag == \"IGN\":\n",
    "            # end any ongoing span\n",
    "            close_current()\n",
    "            continue\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            # close previous slot, start new\n",
    "            close_current()\n",
    "            slot = tag[2:]\n",
    "            if slot in slots:\n",
    "                current_slot = slot\n",
    "                current_tokens = [tok]\n",
    "            else:\n",
    "                # unknown slot label, ignore\n",
    "                current_slot = None\n",
    "                current_tokens = []\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            slot = tag[2:]\n",
    "            if current_slot == slot:\n",
    "                current_tokens.append(tok)\n",
    "            else:\n",
    "                # if I- without B- or different slot, treat as new B-\n",
    "                close_current()\n",
    "                if slot in slots:\n",
    "                    current_slot = slot\n",
    "                    current_tokens = [tok]\n",
    "\n",
    "    # close last\n",
    "    close_current()\n",
    "    return slots\n",
    "\n",
    "def format_slots(slot_dict):\n",
    "    return \"{ \" + \", \".join([f'\"{k}\":\"{slot_dict.get(k, \"\")}\"' for k in SLOT_NAMES]) + \" }\"\n",
    "\n",
    "# 1) Get raw predictions from BERT on the test set\n",
    "pred_output = trainer.predict(test_dataset)\n",
    "logits = pred_output.predictions\n",
    "pred_ids = np.argmax(logits, axis=-1)\n",
    "true_ids = pred_output.label_ids\n",
    "\n",
    "# 2) Decode BIO sequences (token-level) from IDs\n",
    "pred_bio_seqs = []\n",
    "gold_bio_seqs = []\n",
    "\n",
    "for i in range(len(test_tokens)):\n",
    "    word_ids = test_encodings.word_ids(batch_index=i)\n",
    "    bio_pred = []\n",
    "    bio_gold = []\n",
    "    prev_word = None\n",
    "\n",
    "    for p_id, t_id, w_id in zip(pred_ids[i], true_ids[i], word_ids):\n",
    "        if w_id is None or t_id == -100:\n",
    "            continue\n",
    "        if w_id != prev_word:\n",
    "            bio_pred.append(id2label[p_id])\n",
    "            bio_gold.append(id2label[t_id])\n",
    "        prev_word = w_id\n",
    "\n",
    "    pred_bio_seqs.append(bio_pred)\n",
    "    gold_bio_seqs.append(bio_gold)\n",
    "\n",
    "# 3) Convert BIO sequences into slot dictionaries\n",
    "pred_slots_list = [\n",
    "    bio_to_slots(tokens, tags)\n",
    "    for tokens, tags in zip(test_tokens, pred_bio_seqs)\n",
    "]\n",
    "\n",
    "gold_slots_list = [\n",
    "    bio_to_slots(tokens, tags)\n",
    "    for tokens, tags in zip(test_tokens, gold_bio_seqs)\n",
    "]\n",
    "\n",
    "# 4) Compute Slot-level Precision / Recall / F1 based on exact string match\n",
    "TP = FP = FN = 0\n",
    "\n",
    "for gold, pred in zip(gold_slots_list, pred_slots_list):\n",
    "    for slot in SLOT_NAMES:\n",
    "        g = (gold.get(slot) or \"\").strip()\n",
    "        p = (pred.get(slot) or \"\").strip()\n",
    "\n",
    "        if not g and not p:\n",
    "            continue  # ignore both empty\n",
    "\n",
    "        if p and g:\n",
    "            if p == g:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "                FN += 1\n",
    "        elif p and not g:\n",
    "            FP += 1\n",
    "        elif g and not p:\n",
    "            FN += 1\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "slot_f1   = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "# 5) Compute Frame-Validity % (here: frame is \"valid\" if at least one core slot is non-empty)\n",
    "valid_frames = 0\n",
    "for pred in pred_slots_list:\n",
    "    if any((pred.get(s) or \"\").strip() for s in [\"ACTOR\", \"ACTION\", \"OBJECT\"]):\n",
    "        valid_frames += 1\n",
    "\n",
    "frame_valid_pct = 100.0 * valid_frames / len(pred_slots_list) if len(pred_slots_list) > 0 else 0.0\n",
    "\n",
    "print(f\"Slot-F1: {slot_f1:.3f}\")\n",
    "print(f\"Frame-Validity %: {frame_valid_pct:.1f}%\")\n",
    "\n",
    "# 6) Print a few nice text-level examples\n",
    "num_examples = 5\n",
    "for i in range(num_examples):\n",
    "    text = test_df.iloc[i][\"text\"]\n",
    "    print(\"\\nText:\", text)\n",
    "    print(\"Pred:\", format_slots(pred_slots_list[i]))\n",
    "    print(\"Gold:\", format_slots(gold_slots_list[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab18e39",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "Track 0 â€“ Step 2A (T0.2A) is about establishing the official BERT baseline using the new fixed Sprint 4 train/dev/test split. Since Sprint 3 used random or inconsistent splits, the earlier metrics were not fully reliable. By retraining BERT on the fixed 228-sample training set and evaluating on the fixed dev and test sets, we now have stable and fair metrics that we can compare all future Sprint 4 experiments against.\n",
    "\n",
    "On the fixed test set, BERT achieved Slot-F1 â‰ˆ 0.507 and Frame-Validity â‰ˆ 92%. Slot-F1 measures how accurately the model extracts the correct ACTOR, ACTION, OBJECT, etc., while Frame-Validity measures whether the model can produce at least a minimally valid frame (with at least one core slot filled). These numbers represent the true baseline performance of BERT before making any improvements. We will use these results to judge whether data cleaning, better tagging, or other pipeline adjustments improve the model in later Sprint 4 tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d1a6c",
   "metadata": {},
   "source": [
    "### T0.2B: T5 Baseline on Fixed Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c45f54",
   "metadata": {},
   "source": [
    "### Cell 1: Load the fixed split (same as BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0219481d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 49, 50)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"idea_annotator_sprint4_split_fixed.jsonl\", lines=True)\n",
    "\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "dev_df   = df[df[\"split\"] == \"dev\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "len(train_df), len(dev_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7d85a",
   "metadata": {},
   "source": [
    "### Cell 2: Prepare T5 inputs and targets\n",
    "\n",
    "For T5:\n",
    "\n",
    "Input = the raw text\n",
    "\n",
    "Target = a clean JSON string built from target_json with keys\n",
    "ACTOR, ACTION, OBJECT, LOCATION, TIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a597e27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 49, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "SLOT_NAMES = [\"ACTOR\", \"ACTION\", \"OBJECT\", \"LOCATION\", \"TIME\"]\n",
    "\n",
    "def json_to_str(j):\n",
    "    # j is a Python dict from the target_json column\n",
    "    # Ensure all 5 slots exist and convert to a JSON string\n",
    "    out = {}\n",
    "    for slot in SLOT_NAMES:\n",
    "        val = j.get(slot, \"\")\n",
    "        if val is None:\n",
    "            val = \"\"\n",
    "        out[slot] = val\n",
    "    return json.dumps(out, ensure_ascii=False)\n",
    "\n",
    "# Prepare inputs (texts) and targets (JSON strings) for T5\n",
    "train_inputs = train_df[\"text\"].tolist()\n",
    "dev_inputs   = dev_df[\"text\"].tolist()\n",
    "test_inputs  = test_df[\"text\"].tolist()\n",
    "\n",
    "train_targets = [json_to_str(j) for j in train_df[\"target_json\"]]\n",
    "dev_targets   = [json_to_str(j) for j in dev_df[\"target_json\"]]\n",
    "test_targets  = [json_to_str(j) for j in test_df[\"target_json\"]]\n",
    "\n",
    "len(train_inputs), len(dev_inputs), len(test_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65542c",
   "metadata": {},
   "source": [
    "### Cell 3: Load T5 tokenizer + model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adfe4658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp39-cp39-win_amd64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 10.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab8b290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f01cd4",
   "metadata": {},
   "source": [
    "### Cell 4: Tokenize inputs + targets for T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ea904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_input_len = 256\n",
    "max_target_len = 128\n",
    "\n",
    "# Encode input texts\n",
    "train_encodings = t5_tokenizer(\n",
    "    train_inputs,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_input_len\n",
    ")\n",
    "\n",
    "dev_encodings = t5_tokenizer(\n",
    "    dev_inputs,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_input_len\n",
    ")\n",
    "\n",
    "test_encodings = t5_tokenizer(\n",
    "    test_inputs,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_input_len\n",
    ")\n",
    "\n",
    "# Encode target JSON strings\n",
    "with t5_tokenizer.as_target_tokenizer():\n",
    "    train_labels_enc = t5_tokenizer(\n",
    "        train_targets,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_target_len\n",
    "    )\n",
    "    dev_labels_enc = t5_tokenizer(\n",
    "        dev_targets,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_target_len\n",
    "    )\n",
    "    test_labels_enc = t5_tokenizer(\n",
    "        test_targets,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_target_len\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4067fc",
   "metadata": {},
   "source": [
    "### Cell 5: Build T5 Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f8ba09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 49, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class T5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[\"input_ids\"][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Build datasets\n",
    "train_dataset = T5Dataset(train_encodings, train_labels_enc)\n",
    "dev_dataset   = T5Dataset(dev_encodings, dev_labels_enc)\n",
    "test_dataset  = T5Dataset(test_encodings, test_labels_enc)\n",
    "\n",
    "len(train_dataset), len(dev_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6034f8",
   "metadata": {},
   "source": [
    "### Cell 6: Create TrainingArguments + Trainer for T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32d85c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_s4_outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    ")\n",
    "\n",
    "# Data collator for seq2seq (same idea as Sprint 3)\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=t5_tokenizer,\n",
    "    model=t5_model\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e13a80",
   "metadata": {},
   "source": [
    "### Cell 7: Train T5 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6cc3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\data\\data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 15:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.375200</td>\n",
       "      <td>2.078088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.073500</td>\n",
       "      <td>1.177325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.332000</td>\n",
       "      <td>0.883351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.040900</td>\n",
       "      <td>0.739457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.698634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=285, training_loss=1.8152378684596011, metrics={'train_runtime': 960.2308, 'train_samples_per_second': 1.187, 'train_steps_per_second': 0.297, 'total_flos': 25011799326720.0, 'train_loss': 1.8152378684596011, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a97a9",
   "metadata": {},
   "source": [
    "### Cell 8: Evaluate T5 on dev + test (loss only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e96e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV BASELINE (loss metrics): {'eval_loss': 0.6986342668533325, 'eval_runtime': 5.5822, 'eval_samples_per_second': 8.778, 'eval_steps_per_second': 2.329, 'epoch': 5.0}\n",
      "TEST BASELINE (loss metrics): {'eval_loss': 0.6277286410331726, 'eval_runtime': 7.2534, 'eval_samples_per_second': 6.893, 'eval_steps_per_second': 1.792, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on dev and test with the standard Trainer metrics (loss)\n",
    "dev_results = trainer.evaluate(eval_dataset=dev_dataset)\n",
    "print(\"DEV BASELINE (loss metrics):\", dev_results)\n",
    "\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"TEST BASELINE (loss metrics):\", test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851b1c7",
   "metadata": {},
   "source": [
    "### Cell 9: Generate T5 predictions, compute Slot-F1 & Frame-Validity, show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d866ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t5_model.eval()\n",
    "\n",
    "pred_strs = []\n",
    "batch_size = 8\n",
    "device = t5_model.device\n",
    "\n",
    "for i in range(0, len(test_inputs), batch_size):\n",
    "    batch_texts = test_inputs[i:i+batch_size]\n",
    "\n",
    "    enc = t5_tokenizer(\n",
    "        batch_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_input_len\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = t5_model.generate(\n",
    "            **enc,\n",
    "            max_length=max_target_len,\n",
    "            num_beams=1   # FASTEST\n",
    "        )\n",
    "\n",
    "    batch_preds = [\n",
    "        t5_tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        for ids in out_ids\n",
    "    ]\n",
    "\n",
    "    pred_strs.extend(batch_preds)\n",
    "\n",
    "len(pred_strs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515ce84",
   "metadata": {},
   "source": [
    "### Cell 9b (FULL evaluation + examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbf2d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot-F1: 0.000\n",
      "Frame-Validity %: 0.0%\n",
      "\n",
      "Text: When a character delivers a speech so powerful that it emotionally moves the others to take action and not lose hope\n",
      "Pred: { \"ACTOR\":\"\", \"ACTION\":\"\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"character\", \"ACTION\":\"delivers\", \"OBJECT\":\"speech\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "\n",
      "Text: A character who very rarely or never shows any emotion\n",
      "Pred: { \"ACTOR\":\"\", \"ACTION\":\"\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"character\", \"ACTION\":\"shows\", \"OBJECT\":\"emotion\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "\n",
      "Text: Limited color palette on purpose\n",
      "Pred: { \"ACTOR\":\"\", \"ACTION\":\"\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"purpose\", \"ACTION\":\"\", \"OBJECT\":\"color palette\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "\n",
      "Text: A character comes home and finds that they cannot fit in there anymore\n",
      "Pred: { \"ACTOR\":\"\", \"ACTION\":\"\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"character\", \"ACTION\":\"comes\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "\n",
      "Text: The best (or only) way to get rid of something is burning it\n",
      "Pred: { \"ACTOR\":\"\", \"ACTION\":\"\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n",
      "Gold: { \"ACTOR\":\"way\", \"ACTION\":\"burning\", \"OBJECT\":\"\", \"LOCATION\":\"\", \"TIME\":\"\" }\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- Helper: Convert predicted JSON string to dict with 5 slots ---\n",
    "def clean_pred_json(pred_str):\n",
    "    try:\n",
    "        data = json.loads(pred_str)\n",
    "        if not isinstance(data, dict):\n",
    "            return {s: \"\" for s in SLOT_NAMES}\n",
    "    except:\n",
    "        return {s: \"\" for s in SLOT_NAMES}\n",
    "\n",
    "    out = {}\n",
    "    for s in SLOT_NAMES:\n",
    "        val = data.get(s, \"\")\n",
    "        if val is None:\n",
    "            val = \"\"\n",
    "        out[s] = str(val)\n",
    "    return out\n",
    "\n",
    "# --- Convert gold into standard dict format as well ---\n",
    "def clean_gold_json(gold_dict):\n",
    "    out = {}\n",
    "    for s in SLOT_NAMES:\n",
    "        val = gold_dict.get(s, \"\")\n",
    "        if val is None:\n",
    "            val = \"\"\n",
    "        out[s] = str(val)\n",
    "    return out\n",
    "\n",
    "# Parse predicted + gold slots\n",
    "pred_slots_list = [clean_pred_json(p) for p in pred_strs]\n",
    "gold_slots_list = [clean_gold_json(j) for j in test_df[\"target_json\"]]\n",
    "\n",
    "# --- Compute Slot-level F1 score ---\n",
    "TP = FP = FN = 0\n",
    "\n",
    "for gold, pred in zip(gold_slots_list, pred_slots_list):\n",
    "    for slot in SLOT_NAMES:\n",
    "        g = gold[slot].strip()\n",
    "        p = pred[slot].strip()\n",
    "\n",
    "        if g == \"\" and p == \"\":\n",
    "            continue\n",
    "\n",
    "        if g != \"\" and p != \"\":\n",
    "            if g == p:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "                FN += 1\n",
    "        elif g != \"\" and p == \"\":\n",
    "            FN += 1\n",
    "        elif g == \"\" and p != \"\":\n",
    "            FP += 1\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "slot_f1   = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "# --- Compute Frame Validity (at least one of ACTOR/ACTION/OBJECT filled) ---\n",
    "valid_frames = 0\n",
    "for pred in pred_slots_list:\n",
    "    if any(pred[s].strip() for s in [\"ACTOR\", \"ACTION\", \"OBJECT\"]):\n",
    "        valid_frames += 1\n",
    "\n",
    "frame_valid_pct = 100 * valid_frames / len(pred_slots_list)\n",
    "\n",
    "print(f\"Slot-F1: {slot_f1:.3f}\")\n",
    "print(f\"Frame-Validity %: {frame_valid_pct:.1f}%\")\n",
    "\n",
    "# --- Print example outputs ---\n",
    "def fmt(d):\n",
    "    return \"{ \" + \", \".join([f'\"{s}\":\"{d[s]}\"' for s in SLOT_NAMES]) + \" }\"\n",
    "\n",
    "num_examples = 5\n",
    "for i in range(num_examples):\n",
    "    print(\"\\nText:\", test_df.iloc[i][\"text\"])\n",
    "    print(\"Pred:\", fmt(pred_slots_list[i]))\n",
    "    print(\"Gold:\", fmt(gold_slots_list[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd1d2a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example 1 ===\n",
      "RAW PRED: '\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"'\n",
      "GOLD STR: {\"ACTOR\": \"character\", \"ACTION\": \"delivers\", \"OBJECT\": \"speech\", \"LOCATION\": \"\", \"TIME\": \"\"}\n",
      "\n",
      "=== Example 2 ===\n",
      "RAW PRED: '\"ACTOR\": \"character\", \"ACTION\": \"\"'\n",
      "GOLD STR: {\"ACTOR\": \"character\", \"ACTION\": \"shows\", \"OBJECT\": \"emotion\", \"LOCATION\": \"\", \"TIME\": \"\"}\n",
      "\n",
      "=== Example 3 ===\n",
      "RAW PRED: '\"Long color palette\": \"limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"Limited color palette\", \"'\n",
      "GOLD STR: {\"ACTOR\": \"purpose\", \"ACTION\": \"\", \"OBJECT\": \"color palette\", \"LOCATION\": \"\", \"TIME\": \"\"}\n",
      "\n",
      "=== Example 4 ===\n",
      "RAW PRED: \"Un personnage rejoigne et dÃ©couvre qu'il ne s'y trouve plus et qu'il ne s'y trouve plus qu'un personnage.\"\n",
      "GOLD STR: {\"ACTOR\": \"character\", \"ACTION\": \"comes\", \"OBJECT\": \"\", \"LOCATION\": \"\", \"TIME\": \"\"}\n",
      "\n",
      "=== Example 5 ===\n",
      "RAW PRED: '\"ACTOR\": \"act\", \"ACTION\": \"\"'\n",
      "GOLD STR: {\"ACTOR\": \"way\", \"ACTION\": \"burning\", \"OBJECT\": \"\", \"LOCATION\": \"\", \"TIME\": \"\"}\n"
     ]
    }
   ],
   "source": [
    "# Inspect first 5 raw T5 outputs + their gold JSON strings\n",
    "for i in range(5):\n",
    "    print(f\"\\n=== Example {i+1} ===\")\n",
    "    print(\"RAW PRED:\", repr(pred_strs[i]))\n",
    "    print(\"GOLD STR:\", test_targets[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804b799",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "\n",
    "Purpose:\n",
    "Establish a fair baseline for T5 using the new, raw Sprint 4 dataset with fixed train/dev/test split.\n",
    "\n",
    "Setup:\n",
    "\n",
    "Model: T5-small\n",
    "\n",
    "Input: raw text\n",
    "\n",
    "Target: strict JSON (5 slots: ACTOR, ACTION, OBJECT, LOCATION, TIME)\n",
    "\n",
    "Train size: 228\n",
    "\n",
    "Dev size: 49\n",
    "\n",
    "Test size: 50\n",
    "\n",
    "Results (Test Set):\n",
    "\n",
    "Slot-F1: 0.000\n",
    "\n",
    "Frame-Validity: 0.0%\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "T5 is not able to generate valid JSON reliably under the Sprint 4 setup.\n",
    "\n",
    "Many predictions are empty, malformed, or not valid JSON.\n",
    "\n",
    "This does not mean the model is broken â€” it means the task (raw text â†’ strict JSON) is too hard for T5-small with only 228 examples.\n",
    "\n",
    "This creates a weak but honest baseline, which we will improve in later tracks (Track 1, Track 2, Track 3 of Sprint 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fffb9",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d09f79",
   "metadata": {},
   "source": [
    "### T0.3 Error Log (Sprint 4 Baseline)\n",
    "\n",
    "Purpose:\n",
    "Before starting any improvement experiments in Sprint 4, we need to understand how the models (BERT and T5) are making mistakes.\n",
    "T0.3 creates an error log â€” a table that records for each test sentence:\n",
    "\n",
    "the original text\n",
    "\n",
    "the gold slots (ACTOR, ACTION, OBJECT, LOCATION, TIME)\n",
    "\n",
    "the modelâ€™s predicted slots\n",
    "\n",
    "how many slots it got correct\n",
    "\n",
    "whether the prediction was empty or partially correct\n",
    "\n",
    "This is like a report card showing what the model is struggling with.\n",
    "Later, when we apply improvements (Track 1, 2, 3), we can compare:\n",
    "\n",
    "Before improvements (T0.3)\n",
    "vs\n",
    "After improvements\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6caa37",
   "metadata": {},
   "source": [
    "### T0.3 has two parts:\n",
    "\n",
    "**T0.3A â€” BERT Error Log**\n",
    "\n",
    "Use BERTâ€™s baseline predictions (BIO â†’ slots).\n",
    "\n",
    "Build a DataFrame showing BERT mistakes.\n",
    "\n",
    "**T0.3B â€” T5 Error Log**\n",
    "\n",
    "Use T5â€™s baseline predictions (JSON â†’ slots).\n",
    "\n",
    "Build a DataFrame showing T5 mistakes.\n",
    "\n",
    "Both logs help us identify:\n",
    "\n",
    "- frequent error patterns\n",
    "\n",
    "- difficult sentences\n",
    "\n",
    "- which slots fail the most\n",
    "\n",
    "- model weaknesses to target in Sprint 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fe0ae",
   "metadata": {},
   "source": [
    "### T0.3A (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52542a08",
   "metadata": {},
   "source": [
    "### Cell 1: Re-load BERT model and tokenizer\n",
    "\n",
    "The kernal has restarted but training earlier should be saved in the checkpoints. \n",
    "This cell will help us :\n",
    "\n",
    "- Reload the BERT tokenizer\n",
    "\n",
    "- Find the latest checkpoint in ./bert_s4_outputs\n",
    "\n",
    "- Load that checkpoint as the BERT model we will use for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1088ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian Chia\\.conda\\envs\\it3386\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading BERT checkpoint: ./bert_s4_outputs\\checkpoint-145\n",
      "Labels in this model: {0: 'B-ACTION', 1: 'B-ACTOR', 2: 'B-LOCATION', 3: 'B-OBJECT', 4: 'I-ACTION', 5: 'I-ACTOR', 6: 'I-LOCATION', 7: 'I-OBJECT', 8: 'O'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "\n",
    "# 1) Load tokenizer (same as before)\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 2) Find latest BERT checkpoint from Sprint 4 training\n",
    "checkpoint_dirs = glob.glob(\"./bert_s4_outputs/checkpoint-*\")\n",
    "\n",
    "if not checkpoint_dirs:\n",
    "    print(\"âŒ No BERT checkpoints found in ./bert_s4_outputs. \"\n",
    "          \"You may need to re-run the BERT training cells from T0.2A.\")\n",
    "else:\n",
    "    # Sort by step number and take the last (most recent)\n",
    "    def extract_step(path):\n",
    "        # path looks like './bert_s4_outputs/checkpoint-285'\n",
    "        name = os.path.basename(path)\n",
    "        step_str = name.split(\"-\")[-1]\n",
    "        return int(step_str) if step_str.isdigit() else -1\n",
    "\n",
    "    checkpoint_dirs = sorted(checkpoint_dirs, key=extract_step)\n",
    "    best_ckpt = checkpoint_dirs[-1]\n",
    "    print(\"âœ… Loading BERT checkpoint:\", best_ckpt)\n",
    "\n",
    "    bert_model = BertForTokenClassification.from_pretrained(best_ckpt)\n",
    "    bert_model.eval()\n",
    "\n",
    "    # id2label mapping (e.g. 0 -> B-ACTION, 1 -> B-ACTOR, etc.)\n",
    "    id2label = bert_model.config.id2label\n",
    "    label2id = bert_model.config.label2id\n",
    "\n",
    "    print(\"Labels in this model:\", id2label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd874ee",
   "metadata": {},
   "source": [
    "### Cell 2: Run BERT on test set and convert to slot predictions\n",
    "\n",
    "This cell will:\n",
    "- Tokenize the test sentences for BERT\n",
    "\n",
    "- Run the loaded BERT model to get predictions\n",
    "\n",
    "- Convert predicted BIO tags â†’ slot spans\n",
    "\n",
    "- Build:\n",
    "\n",
    "1) bert_gold_slots_list\n",
    "\n",
    "2) bert_pred_slots_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba4798c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We'll use these 4 slots for BERT (TIME not in label set)\n",
    "SLOT_NAMES_BERT = [\"ACTOR\", \"ACTION\", \"OBJECT\", \"LOCATION\"]\n",
    "\n",
    "# Helper: convert BIO tags & tokens into slot dictionary\n",
    "def bio_to_slots(tokens, tags, slot_names=SLOT_NAMES_BERT):\n",
    "    slots = {s: \"\" for s in slot_names}\n",
    "    current_slot = None\n",
    "    current_tokens = []\n",
    "\n",
    "    def close_current():\n",
    "        nonlocal current_slot, current_tokens\n",
    "        if current_slot is not None and current_tokens and slots[current_slot] == \"\":\n",
    "            slots[current_slot] = \" \".join(current_tokens)\n",
    "        current_slot = None\n",
    "        current_tokens = []\n",
    "\n",
    "    for tok, tag in zip(tokens, tags):\n",
    "        if tag == \"O\":\n",
    "            close_current()\n",
    "            continue\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            close_current()\n",
    "            slot = tag[2:]\n",
    "            if slot in slots:\n",
    "                current_slot = slot\n",
    "                current_tokens = [tok]\n",
    "            else:\n",
    "                current_slot = None\n",
    "                current_tokens = []\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            slot = tag[2:]\n",
    "            if current_slot == slot:\n",
    "                current_tokens.append(tok)\n",
    "            else:\n",
    "                close_current()\n",
    "                if slot in slots:\n",
    "                    current_slot = slot\n",
    "                    current_tokens = [tok]\n",
    "\n",
    "    close_current()\n",
    "    return slots\n",
    "\n",
    "# 1) Prepare test tokens (simple whitespace tokenization)\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "test_tokens_bert = [t.split() for t in test_texts]\n",
    "\n",
    "# 2) Tokenize with BERT (word-level alignment)\n",
    "bert_encodings = bert_tokenizer(\n",
    "    test_tokens_bert,\n",
    "    is_split_into_words=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 3) Move model & data to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "input_ids = bert_encodings[\"input_ids\"].to(device)\n",
    "attention_mask = bert_encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "# 4) Run BERT to get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # (batch, seq_len, num_labels)\n",
    "    pred_ids = logits.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "# 5) Convert predicted IDs â†’ BIO tags per word, then â†’ slots\n",
    "bert_pred_slots_list = []\n",
    "bert_gold_slots_list = []\n",
    "\n",
    "for i in range(len(test_tokens_bert)):\n",
    "    word_ids = bert_encodings.word_ids(batch_index=i)\n",
    "    bio_preds = []\n",
    "    \n",
    "    prev_word = None\n",
    "    for token_idx, w_id in enumerate(word_ids):\n",
    "        if w_id is None:\n",
    "            continue\n",
    "        if w_id != prev_word:\n",
    "            label_id = int(pred_ids[i][token_idx])\n",
    "            tag = id2label[label_id]\n",
    "            bio_preds.append(tag)\n",
    "            prev_word = w_id\n",
    "\n",
    "    # Predicted slots from BIO\n",
    "    pred_slots = bio_to_slots(test_tokens_bert[i], bio_preds)\n",
    "    bert_pred_slots_list.append(pred_slots)\n",
    "\n",
    "    # Gold slots from test_df[\"target_json\"]\n",
    "    gold_json = test_df.iloc[i][\"target_json\"]\n",
    "    gold_slots = {s: (gold_json.get(s, \"\") or \"\") for s in SLOT_NAMES_BERT}\n",
    "    bert_gold_slots_list.append(gold_slots)\n",
    "\n",
    "len(bert_pred_slots_list), len(bert_gold_slots_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd379b2",
   "metadata": {},
   "source": [
    "### Cell 3: Build BERT error log DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02bdcb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold_ACTOR</th>\n",
       "      <th>pred_ACTOR</th>\n",
       "      <th>gold_ACTION</th>\n",
       "      <th>pred_ACTION</th>\n",
       "      <th>gold_OBJECT</th>\n",
       "      <th>pred_OBJECT</th>\n",
       "      <th>gold_LOCATION</th>\n",
       "      <th>pred_LOCATION</th>\n",
       "      <th>num_correct_slots</th>\n",
       "      <th>any_core_pred</th>\n",
       "      <th>all_empty_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When a character delivers a speech so powerful...</td>\n",
       "      <td>character</td>\n",
       "      <td>character</td>\n",
       "      <td>delivers</td>\n",
       "      <td>delivers</td>\n",
       "      <td>speech</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A character who very rarely or never shows any...</td>\n",
       "      <td>character</td>\n",
       "      <td>character</td>\n",
       "      <td>shows</td>\n",
       "      <td>shows</td>\n",
       "      <td>emotion</td>\n",
       "      <td>emotion</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Limited color palette on purpose</td>\n",
       "      <td>purpose</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>color palette</td>\n",
       "      <td>Limited color palette</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A character comes home and finds that they can...</td>\n",
       "      <td>character</td>\n",
       "      <td>character</td>\n",
       "      <td>comes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best (or only) way to get rid of something...</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "      <td>burning</td>\n",
       "      <td>burning</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text gold_ACTOR pred_ACTOR  \\\n",
       "0  When a character delivers a speech so powerful...  character  character   \n",
       "1  A character who very rarely or never shows any...  character  character   \n",
       "2                   Limited color palette on purpose    purpose              \n",
       "3  A character comes home and finds that they can...  character  character   \n",
       "4  The best (or only) way to get rid of something...        way        way   \n",
       "\n",
       "  gold_ACTION pred_ACTION    gold_OBJECT            pred_OBJECT gold_LOCATION  \\\n",
       "0    delivers    delivers         speech                                        \n",
       "1       shows       shows        emotion                emotion                 \n",
       "2                          color palette  Limited color palette                 \n",
       "3       comes                                                                   \n",
       "4     burning     burning                                                       \n",
       "\n",
       "  pred_LOCATION  num_correct_slots  any_core_pred  all_empty_pred  \n",
       "0                                2           True           False  \n",
       "1                                3           True           False  \n",
       "2                                0           True           False  \n",
       "3                                1           True           False  \n",
       "4                                2           True           False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    text = test_df.iloc[i][\"text\"]\n",
    "    gold = bert_gold_slots_list[i]   # dict with ACTOR/ACTION/OBJECT/LOCATION\n",
    "    pred = bert_pred_slots_list[i]   # dict with ACTOR/ACTION/OBJECT/LOCATION\n",
    "\n",
    "    row = {\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "    correct_count = 0\n",
    "\n",
    "    for slot in SLOT_NAMES_BERT:  # [\"ACTOR\", \"ACTION\", \"OBJECT\", \"LOCATION\"]\n",
    "        g = (gold.get(slot, \"\") or \"\").strip()\n",
    "        p = (pred.get(slot, \"\") or \"\").strip()\n",
    "\n",
    "        row[f\"gold_{slot}\"] = g\n",
    "        row[f\"pred_{slot}\"] = p\n",
    "\n",
    "        if g != \"\" and p == g:\n",
    "            correct_count += 1\n",
    "\n",
    "    row[\"num_correct_slots\"] = correct_count\n",
    "\n",
    "    any_core_pred = any(\n",
    "        (pred.get(s, \"\") or \"\").strip()\n",
    "        for s in [\"ACTOR\", \"ACTION\", \"OBJECT\"]\n",
    "    )\n",
    "    all_empty_pred = not any(\n",
    "        (pred.get(s, \"\") or \"\").strip()\n",
    "        for s in SLOT_NAMES_BERT\n",
    "    )\n",
    "\n",
    "    row[\"any_core_pred\"] = any_core_pred\n",
    "    row[\"all_empty_pred\"] = all_empty_pred\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "bert_error_log_df = pd.DataFrame(rows)\n",
    "bert_error_log_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6301179",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "### T0.3A â€” BERT Error Log (Sprint 4 Baseline)\n",
    "\n",
    "**Purpose:**  \n",
    "After training the BERT BIO tagging model on the Sprint 4 fixed split, I generated predictions on the 50 test sentences and converted them back into slot values (ACTOR, ACTION, OBJECT, LOCATION).  \n",
    "T0.3A creates an *error log* so I can see, for each test sentence:\n",
    "\n",
    "- the original text,\n",
    "- the gold slots from `target_json`,\n",
    "- BERTâ€™s predicted slots,\n",
    "- how many slots were correct (`num_correct_slots`),\n",
    "- whether BERT predicted any core slot at all.\n",
    "\n",
    "**What I see:**  \n",
    "- Many rows have matching `gold_` and `pred_` values for ACTOR / ACTION / OBJECT, which matches the earlier BERT baseline metrics (Slot-F1 â‰ˆ 0.50, Frame-Validity â‰ˆ 92%).  \n",
    "- Some rows show typical errors, e.g. BERT chooses a slightly different span (â€œLimited color paletteâ€ instead of â€œcolor paletteâ€), or misses a slot completely.\n",
    "\n",
    "This error log will be reused later to explain common BERT error patterns and to compare against any Sprint 4 improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afb96e",
   "metadata": {},
   "source": [
    "### T0.3B â€“ T5 Error Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a381aa",
   "metadata": {},
   "source": [
    "he goal is to build a table (DataFrame) that shows exactly how the T5 baseline failed for each of the 50 test sentences.\n",
    "\n",
    "To do that, we need to:\n",
    "\n",
    "1) Load the Sprint 4 fixed split dataset again\n",
    "(so we have test_df with the same 50 test examples)\n",
    "\n",
    "2) Extract the gold slots\n",
    "from each rowâ€™s target_json (ACTOR, ACTION, etc.)\n",
    "\n",
    "3) Load the T5 predictions we previously generated (pred_strs)\n",
    "\n",
    "4) Parse the predictions into slot dictionaries\n",
    "(mostly empty because T5 couldnâ€™t produce valid JSON)\n",
    "\n",
    "5) Combine everything into an error log row-by-row:\n",
    "\n",
    "- gold slots\n",
    "\n",
    "- predicted slots\n",
    "\n",
    "- count how many slots match\n",
    "\n",
    "- mark if T5 predicted anything at all\n",
    "\n",
    "- mark if everything was empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb55cb",
   "metadata": {},
   "source": [
    "### Cell 1: Load dataset + recreate train/dev/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "430e94b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 49, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Sprint 4 fixed split dataset\n",
    "df = pd.read_json(\"idea_annotator_sprint4_split_fixed.jsonl\", lines=True)\n",
    "\n",
    "# Recreate train/dev/test from the fixed split\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "dev_df   = df[df[\"split\"] == \"dev\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "len(train_df), len(dev_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07086c2f",
   "metadata": {},
   "source": [
    "### Cell 2: Define SLOT_NAMES + build gold_slots_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f176e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ACTOR': 'character',\n",
       "  'ACTION': 'delivers',\n",
       "  'OBJECT': 'speech',\n",
       "  'LOCATION': '',\n",
       "  'TIME': ''},\n",
       " {'ACTOR': 'character',\n",
       "  'ACTION': 'shows',\n",
       "  'OBJECT': 'emotion',\n",
       "  'LOCATION': '',\n",
       "  'TIME': ''},\n",
       " {'ACTOR': 'purpose',\n",
       "  'ACTION': '',\n",
       "  'OBJECT': 'color palette',\n",
       "  'LOCATION': '',\n",
       "  'TIME': ''}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T0.3B â€“ Cell 2: define slot names and build gold_slots_list from test_df\n",
    "\n",
    "SLOT_NAMES = [\"ACTOR\", \"ACTION\", \"OBJECT\", \"LOCATION\", \"TIME\"]\n",
    "\n",
    "def normalize_to_slot_dict(raw):\n",
    "    \"\"\"\n",
    "    Take target_json (string or dict) and return a clean dict\n",
    "    with exactly the 5 slot keys. Missing ones become \"\".\n",
    "    \"\"\"\n",
    "    # If it's a string, try to parse as JSON\n",
    "    if isinstance(raw, str):\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            data = {}\n",
    "    elif isinstance(raw, dict):\n",
    "        data = raw\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    # Make sure all 5 slots exist as strings\n",
    "    slots = {}\n",
    "    for slot in SLOT_NAMES:\n",
    "        value = data.get(slot, \"\")\n",
    "        if value is None:\n",
    "            value = \"\"\n",
    "        slots[slot] = str(value).strip()\n",
    "    return slots\n",
    "\n",
    "# Build gold_slots_list for the 50 test examples\n",
    "gold_slots_list = [normalize_to_slot_dict(x) for x in test_df[\"target_json\"]]\n",
    "\n",
    "# Quick sanity check: first 3 gold slot dicts\n",
    "gold_slots_list[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1fde2",
   "metadata": {},
   "source": [
    "### Cell 3: Parse pred_strs into pred_slots_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b4af72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ACTOR': '', 'ACTION': '', 'OBJECT': '', 'LOCATION': '', 'TIME': ''},\n",
       " {'ACTOR': '', 'ACTION': '', 'OBJECT': '', 'LOCATION': '', 'TIME': ''},\n",
       " {'ACTOR': '', 'ACTION': '', 'OBJECT': '', 'LOCATION': '', 'TIME': ''}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T0.3B â€“ Cell 3: parse T5 generated predictions (pred_strs) into pred slot dicts\n",
    "\n",
    "def parse_predicted_json(raw_pred):\n",
    "    \"\"\"\n",
    "    Takes a raw T5 prediction (string).\n",
    "    Attempts to load JSON; if fails, return empty slots.\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_pred, str):\n",
    "        return {slot: \"\" for slot in SLOT_NAMES}\n",
    "\n",
    "    raw_pred = raw_pred.strip()\n",
    "\n",
    "    # Try to load as JSON\n",
    "    try:\n",
    "        data = json.loads(raw_pred)\n",
    "    except Exception:\n",
    "        # invalid JSON â†’ return empty slots\n",
    "        return {slot: \"\" for slot in SLOT_NAMES}\n",
    "    \n",
    "    # Normalize: ensure all 5 slots appear\n",
    "    slots = {}\n",
    "    for slot in SLOT_NAMES:\n",
    "        value = data.get(slot, \"\")\n",
    "        if value is None:\n",
    "            value = \"\"\n",
    "        slots[slot] = str(value).strip()\n",
    "    return slots\n",
    "\n",
    "# Build pred_slots_list from pred_strs\n",
    "pred_slots_list = [parse_predicted_json(p) for p in pred_strs]\n",
    "\n",
    "# Sanity preview of first few predictions\n",
    "pred_slots_list[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434d970",
   "metadata": {},
   "source": [
    "### Create the actual T5 Error Log DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85ed8585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold_ACTOR</th>\n",
       "      <th>gold_ACTION</th>\n",
       "      <th>gold_OBJECT</th>\n",
       "      <th>gold_LOCATION</th>\n",
       "      <th>gold_TIME</th>\n",
       "      <th>pred_ACTOR</th>\n",
       "      <th>pred_ACTION</th>\n",
       "      <th>pred_OBJECT</th>\n",
       "      <th>pred_LOCATION</th>\n",
       "      <th>pred_TIME</th>\n",
       "      <th>num_correct_slots</th>\n",
       "      <th>any_core_pred</th>\n",
       "      <th>all_empty_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When a character delivers a speech so powerful...</td>\n",
       "      <td>character</td>\n",
       "      <td>delivers</td>\n",
       "      <td>speech</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A character who very rarely or never shows any...</td>\n",
       "      <td>character</td>\n",
       "      <td>shows</td>\n",
       "      <td>emotion</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Limited color palette on purpose</td>\n",
       "      <td>purpose</td>\n",
       "      <td></td>\n",
       "      <td>color palette</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A character comes home and finds that they can...</td>\n",
       "      <td>character</td>\n",
       "      <td>comes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best (or only) way to get rid of something...</td>\n",
       "      <td>way</td>\n",
       "      <td>burning</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text gold_ACTOR gold_ACTION  \\\n",
       "0  When a character delivers a speech so powerful...  character    delivers   \n",
       "1  A character who very rarely or never shows any...  character       shows   \n",
       "2                   Limited color palette on purpose    purpose               \n",
       "3  A character comes home and finds that they can...  character       comes   \n",
       "4  The best (or only) way to get rid of something...        way     burning   \n",
       "\n",
       "     gold_OBJECT gold_LOCATION gold_TIME pred_ACTOR pred_ACTION pred_OBJECT  \\\n",
       "0         speech                                                              \n",
       "1        emotion                                                              \n",
       "2  color palette                                                              \n",
       "3                                                                             \n",
       "4                                                                             \n",
       "\n",
       "  pred_LOCATION pred_TIME  num_correct_slots  any_core_pred  all_empty_pred  \n",
       "0                                          2          False            True  \n",
       "1                                          2          False            True  \n",
       "2                                          3          False            True  \n",
       "3                                          3          False            True  \n",
       "4                                          3          False            True  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T0.3B â€“ Cell 4: build the T5 error log DataFrame\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    text = test_df.loc[i, \"text\"]\n",
    "\n",
    "    gold = gold_slots_list[i]\n",
    "    pred = pred_slots_list[i]\n",
    "\n",
    "    # count how many slots match exactly\n",
    "    num_correct = sum(\n",
    "        1 for slot in SLOT_NAMES \n",
    "        if gold.get(slot, \"\") == pred.get(slot, \"\")\n",
    "    )\n",
    "\n",
    "    # any of ACTOR/ACTION/OBJECT non-empty?\n",
    "    any_core_pred = any(pred[slot] != \"\" for slot in [\"ACTOR\", \"ACTION\", \"OBJECT\"])\n",
    "\n",
    "    # all slots empty?\n",
    "    all_empty_pred = all(pred[slot] == \"\" for slot in SLOT_NAMES)\n",
    "\n",
    "    row = {\n",
    "        \"text\": text,\n",
    "    }\n",
    "\n",
    "    # add gold_ columns\n",
    "    for slot in SLOT_NAMES:\n",
    "        row[f\"gold_{slot}\"] = gold[slot]\n",
    "\n",
    "    # add pred_ columns\n",
    "    for slot in SLOT_NAMES:\n",
    "        row[f\"pred_{slot}\"] = pred[slot]\n",
    "\n",
    "    row[\"num_correct_slots\"] = num_correct\n",
    "    row[\"any_core_pred\"] = any_core_pred\n",
    "    row[\"all_empty_pred\"] = all_empty_pred\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "t5_error_log_df = pd.DataFrame(rows)\n",
    "\n",
    "t5_error_log_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b0a19",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802efee",
   "metadata": {},
   "source": [
    "### T0.4 â€“ Cell 1: Collect baseline metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2779974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Slots_used</th>\n",
       "      <th>Slot_F1</th>\n",
       "      <th>Frame_validity_%</th>\n",
       "      <th>#All_empty_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT token tagging</td>\n",
       "      <td>ACTOR, ACTION, OBJECT, LOCATION</td>\n",
       "      <td>0.507</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T5 text-to-JSON</td>\n",
       "      <td>ACTOR, ACTION, OBJECT, LOCATION, TIME</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                             Slots_used  Slot_F1  \\\n",
       "0  BERT token tagging        ACTOR, ACTION, OBJECT, LOCATION    0.507   \n",
       "1     T5 text-to-JSON  ACTOR, ACTION, OBJECT, LOCATION, TIME    0.000   \n",
       "\n",
       "   Frame_validity_%  #All_empty_predictions  \n",
       "0              92.0                       1  \n",
       "1               0.0                      50  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T0.4 â€“ Baseline Summary (FINAL, using the metrics from T0.2A and T0.2B)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Official metrics from T0.2A (BERT) and T0.2B (T5) on Sprint 4 fixed split\n",
    "bert_slot_f1 = 0.507      # from your T0.2A result\n",
    "bert_frame_validity = 92.0\n",
    "\n",
    "t5_slot_f1 = 0.0          # from your T0.2B result\n",
    "t5_frame_validity = 0.0\n",
    "\n",
    "# Use the existing error logs just to count how many predictions were totally empty\n",
    "bert_empty = int(bert_error_log_df[\"all_empty_pred\"].sum())\n",
    "t5_empty = int(t5_error_log_df[\"all_empty_pred\"].sum())\n",
    "\n",
    "summary_for_report = pd.DataFrame({\n",
    "    \"Model\": [\"BERT token tagging\", \"T5 text-to-JSON\"],\n",
    "    \"Slots_used\": [\n",
    "        \"ACTOR, ACTION, OBJECT, LOCATION\",\n",
    "        \"ACTOR, ACTION, OBJECT, LOCATION, TIME\"\n",
    "    ],\n",
    "    \"Slot_F1\": [bert_slot_f1, t5_slot_f1],\n",
    "    \"Frame_validity_%\": [bert_frame_validity, t5_frame_validity],\n",
    "    \"#All_empty_predictions\": [bert_empty, t5_empty]\n",
    "})\n",
    "\n",
    "summary_for_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c864aa",
   "metadata": {},
   "source": [
    "### Track 0 â€“ Baseline Results on Sprint 4 Fixed Split\n",
    "\n",
    "In Track 0, I established two baselines using the Sprint 4 fixed train/dev/test split (228/49/50) and the 5-slot case-frame schema (ACTOR, ACTION, OBJECT, LOCATION, TIME).\n",
    "\n",
    "**BERT token-tagging baseline (T0.2A).**  \n",
    "I fine-tuned a BERT token classification model to predict BIO tags for four core slots (ACTOR, ACTION, OBJECT, LOCATION). On the Sprint 4 test set, BERT achieved **Slot-F1 â‰ˆ 0.507** and **Frame-Validity â‰ˆ 92%**. Slot-F1 measures how accurately the model extracts each slot span (e.g., correct ACTOR phrase, correct OBJECT phrase). Frame-Validity here means the percentage of test sentences where BERT produces at least a *minimally valid* frame, i.e., at least one core slot is filled. The high Frame-Validity shows that BERT almost always predicts something meaningful, but the 0.507 Slot-F1 also shows there is still room to improve the quality and boundaries of the predicted spans.\n",
    "\n",
    "**T5 text-to-JSON baseline (T0.2B).**  \n",
    "I also trained a naive **t5-small** model to directly generate a strict JSON object with all 5 slot keys from the raw idea text. On the same test set, this baseline essentially failed: it frequently produced invalid JSON strings, which became completely empty frames after parsing. As a result, the T5 baseline obtained **Slot-F1 = 0.0** and **Frame-Validity = 0.0%**, with all 50 test examples ending up as empty predictions in the T5 error log.\n",
    "\n",
    "**Summary.**  \n",
    "The comparison shows that:\n",
    "- BERT, even with a simple token-tagging setup, can already recover many slot spans and produce minimally valid frames for most test ideas (92% non-empty frames, Slot-F1 â‰ˆ 0.507).  \n",
    "- A naive text-to-JSON T5 baseline collapses into invalid or empty outputs under the strict JSON format, leading to effectively zero performance.\n",
    "\n",
    "These baselines motivate the next Tracks (1â€“4). Later experiments will focus on improving both **slot accuracy** and **frame validity** by refining the data and labels, improving decoding/post-processing for BERT, and redesigning the T5 setup with better prompts, constraints and JSON-aware training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ee8182",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-it3386] *",
   "language": "python",
   "name": "conda-env-.conda-it3386-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
